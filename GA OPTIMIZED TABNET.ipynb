{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f662e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a26c081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running in Jupyter/IPython environment\n",
      "\n",
      "======================================================================\n",
      "   TABNET TRAINING WITH GENETIC ALGORITHM OPTIMIZATION\n",
      "======================================================================\n",
      "2025-11-13 18:46:53 [INFO] TabNetTrainer:  Random seeds set to 42\n",
      "2025-11-13 18:46:53 [INFO] TabNetTrainer:  Using CPU\n",
      "2025-11-13 18:46:53 [INFO] TabNetTrainer:    Adjusted batch sizes for CPU: batch=64, virtual=32\n",
      "2025-11-13 18:46:53 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-13 18:46:53 [INFO] TabNetTrainer:  TABNET TRAINING EXPERIMENT WITH GA OPTIMIZATION\n",
      "2025-11-13 18:46:53 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-13 18:46:53 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-13 18:46:53 [INFO] TabNetTrainer:  DATA LOADING & PREPROCESSING\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer: Starting: Loading dataset from C:\\Users\\awwal\\Desktop\\MLEA_experiments\\data.csv\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer:    Shape: (569, 33)\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer: âœ“ Completed: Loading dataset from C:\\Users\\awwal\\Desktop\\MLEA_experiments\\data.csv (0.01s)\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer:    Class distribution: Benign=357 (62.7%), Malignant=212 (37.3%)\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer:    Features: 30\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer:    Train set: 369 samples\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer:    Validation set: 86 samples\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer:    Test set: 114 samples\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer: Starting: Feature scaling\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer: âœ“ Completed: Feature scaling (0.00s)\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer: \n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer: DEAP genetic algorithm framework initialized\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer:  ðŸ§¬ GENETIC ALGORITHM OPTIMIZATION\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer: Population Size: 20\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer: Generations: 15\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer: Mutation Rate: 0.3\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer: Crossover Rate: 0.7\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer: \n",
      "ðŸ”„ Starting evolution...\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer:   GENERATION 1/15\n",
      "2025-11-13 18:46:54 [INFO] TabNetTrainer: ============================================================\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 21 and best_val_auc = 0.99711\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 27 and best_val_auc = 0.97569\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 26 and best_val_auc = 0.9919\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 16 and best_val_auc = 1.0\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 28 and best_val_auc = 1.0\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 18 and best_val_auc = 0.99884\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 5 and best_val_auc = 0.99479\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 6 and best_val_auc = 1.0\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 9 and best_val_auc = 0.98119\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 26 and best_val_auc = 0.97627\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 7 and best_val_auc = 0.99884\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 17 and best_val_auc = 0.99306\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 7 and best_val_auc = 0.99132\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 14 and best_val_auc = 0.99826\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 12 and best_val_auc = 1.0\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 17 and best_val_auc = 0.99074\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 7 and best_val_auc = 0.96528\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 27 and best_val_auc = 0.99537\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 9 and best_val_auc = 0.99826\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 29 and best_val_auc = 0.96007\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    Fitness - Max: 0.988498, Avg: 0.923803, Min: 0.743498, Std: 0.064712\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:   GENERATION 2/15\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    Fitness - Max: 0.988498, Avg: 0.965083, Min: 0.931539, Std: 0.015798\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:   GENERATION 3/15\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    Fitness - Max: 0.988498, Avg: 0.979890, Min: 0.966666, Std: 0.005519\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:   GENERATION 4/15\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    Fitness - Max: 0.988498, Avg: 0.986363, Min: 0.977562, Std: 0.004271\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:   GENERATION 5/15\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    Fitness - Max: 0.988498, Avg: 0.988498, Min: 0.988498, Std: 0.000000\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:   GENERATION 6/15\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    Fitness - Max: 0.988498, Avg: 0.988498, Min: 0.988498, Std: 0.000000\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:   GENERATION 7/15\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    Fitness - Max: 0.988498, Avg: 0.988498, Min: 0.988498, Std: 0.000000\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:   GENERATION 8/15\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    Fitness - Max: 0.988498, Avg: 0.988498, Min: 0.988498, Std: 0.000000\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:   GENERATION 9/15\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    Fitness - Max: 0.988498, Avg: 0.988498, Min: 0.988498, Std: 0.000000\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:   GENERATION 10/15\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    Fitness - Max: 0.988498, Avg: 0.988498, Min: 0.988498, Std: 0.000000\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:   GENERATION 11/15\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    Fitness - Max: 0.988498, Avg: 0.988498, Min: 0.988498, Std: 0.000000\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:   GENERATION 12/15\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    Fitness - Max: 0.988498, Avg: 0.988498, Min: 0.988498, Std: 0.000000\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:   GENERATION 13/15\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    Fitness - Max: 0.988498, Avg: 0.988498, Min: 0.988498, Std: 0.000000\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:   GENERATION 14/15\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    Fitness - Max: 0.988498, Avg: 0.988498, Min: 0.988498, Std: 0.000000\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:   GENERATION 15/15\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    Fitness - Max: 0.988498, Avg: 0.988498, Min: 0.988498, Std: 0.000000\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      "======================================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    OPTIMIZATION COMPLETE\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      " Best Fitness Achieved: 0.988498\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      " Optimal Hyperparameters:\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    n_d            : 45\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    n_a            : 22\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    n_steps        : 3\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    gamma          : 1.0709930860090324\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    lambda_sparse  : 0.00634791927697398\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    lr             : 0.015302380271501947\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    momentum       : 0.2725718037717771\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    batch_size     : 960\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    n_independent  : 3\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer:    n_shared       : 1\n",
      "2025-11-13 18:48:32 [INFO] TabNetTrainer: \n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer:  Evolution plot saved: ga_evolution_20251113_184833.png\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer:  MODEL TRAINING\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer: Using optimized hyperparameters from genetic algorithm\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer:    n_d            : 45\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer:    n_a            : 22\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer:    n_steps        : 3\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer:    gamma          : 1.0709930860090324\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer:    lambda_sparse  : 0.00634791927697398\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer:    lr             : 0.015302380271501947\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer:    momentum       : 0.2725718037717771\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer:    batch_size     : 960\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer:    n_independent  : 3\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer:    n_shared       : 1\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer: \n",
      " Training Configuration:\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer:    Max epochs: 50\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer:    Patience: 10\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer:    Batch size: 960\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer:    Virtual batch size: 120\n",
      "2025-11-13 18:48:33 [INFO] TabNetTrainer: Starting: Model training\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_accuracy = 0.98837\n",
      "2025-11-13 18:48:35 [INFO] TabNetTrainer: âœ“ Completed: Model training (1.55s)\n",
      "2025-11-13 18:48:35 [INFO] TabNetTrainer: âœ“ Training completed at epoch 9\n",
      "2025-11-13 18:48:35 [INFO] TabNetTrainer: \n",
      "2025-11-13 18:48:35 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-13 18:48:35 [INFO] TabNetTrainer:   MODEL EVALUATION ON TEST SET\n",
      "2025-11-13 18:48:35 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-13 18:48:35 [INFO] TabNetTrainer: Starting: Model evaluation\n",
      "2025-11-13 18:48:35 [INFO] TabNetTrainer: âœ“ Completed: Model evaluation (0.03s)\n",
      "2025-11-13 18:48:35 [INFO] TabNetTrainer: \n",
      " TEST SET MODEL PERFORMANCE METRICS:\n",
      "   Accuracy:    0.9649\n",
      "   AUC-ROC:     0.9838\n",
      "   Sensitivity: 0.9286\n",
      "   Specificity: 0.9861\n",
      "   Precision:   0.9750\n",
      "   NPV:         0.9595\n",
      "   F1 Score:    0.9512\n",
      "\n",
      " CONFUSION MATRIX:\n",
      "   TN:   71  FP:    1\n",
      "   FN:    3  TP:   39\n",
      "2025-11-13 18:48:35 [INFO] TabNetTrainer: \n",
      " CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign     0.9595    0.9861    0.9726        72\n",
      "   Malignant     0.9750    0.9286    0.9512        42\n",
      "\n",
      "    accuracy                         0.9649       114\n",
      "   macro avg     0.9672    0.9573    0.9619       114\n",
      "weighted avg     0.9652    0.9649    0.9647       114\n",
      "\n",
      "2025-11-13 18:48:35 [INFO] TabNetTrainer: \n",
      "2025-11-13 18:48:35 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-13 18:48:35 [INFO] TabNetTrainer:  GENERATING VISUALIZATIONS\n",
      "2025-11-13 18:48:35 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-13 18:48:35 [ERROR] TabNetTrainer: Failed to create training history plot: 'History' object has no attribute 'get'\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer: \n",
      " Opening 1 plot windows...\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer:    Close plot windows to continue...\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer: âœ“ All visualizations generated and displayed\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer: \n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer: GA results saved: ga_optimization_20251113_184836.pkl\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer: Model saved: tabnet_model_20251113_184836.pth\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer: Results saved: experiment_results_20251113_184836.json\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer:  EXPERIMENT COMPLETED SUCCESSFULLY\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer: Genetic Algorithm Results:\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer:    Best Fitness: 0.988498\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer:    Generations: 15\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer: \n",
      " Final Test Performance:\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer:    Accuracy:    0.9649\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer:    AUC-ROC:     0.9838\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer:    Sensitivity: 0.9286\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer:    Specificity: 0.9861\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer:    F1 Score:    0.9512\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer: \n",
      " Results saved in: tabnet_results_ga/\n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer: \n",
      "2025-11-13 18:48:36 [INFO] TabNetTrainer: \n",
      "\n",
      " Experiment completed! Results stored in 'results' variable\n",
      "\n",
      " GA Optimization Summary:\n",
      "   Best Fitness: 0.988498\n",
      "\n",
      " Optimized Hyperparameters:\n",
      "   n_d            : 45\n",
      "   n_a            : 22\n",
      "   n_steps        : 3\n",
      "   gamma          : 1.0709930860090324\n",
      "   lambda_sparse  : 0.00634791927697398\n",
      "   lr             : 0.015302380271501947\n",
      "   momentum       : 0.2725718037717771\n",
      "   batch_size     : 960\n",
      "   n_independent  : 3\n",
      "   n_shared       : 1\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from enum import Enum\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, roc_auc_score, \n",
    "    classification_report, roc_curve\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# DEAP imports for genetic algorithm\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "# Matplotlib imports for window plotting\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Conditional import for TabNet\n",
    "try:\n",
    "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "    _HAS_TABNET = True\n",
    "except ImportError:\n",
    "    TabNetClassifier = None\n",
    "    _HAS_TABNET = False\n",
    "\n",
    "# =============================================================================\n",
    "# ENUMS AND CONSTANTS\n",
    "# =============================================================================\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "DEFAULT_DROP_COLUMNS = [\"id\", \"ID\", \"Id\", \"patient_id\", \"Unnamed: 0\"]\n",
    "TARGET_CANDIDATES = [\"diagnosis\", \"Diagnosis\", \"target\", \"Target\", \"class\", \"Class\"]\n",
    "\n",
    "# =============================================================================\n",
    "# GENETIC ALGORITHM HYPERPARAMETER BOUNDS\n",
    "# =============================================================================\n",
    "\n",
    "HYPERPARAMETER_BOUNDS = {\n",
    "    'n_d': (8, 64),           # Network width for decision layer\n",
    "    'n_a': (8, 64),           # Network width for attention layer\n",
    "    'n_steps': (3, 10),       # Number of decision steps\n",
    "    'gamma': (1.0, 2.0),      # Relaxation parameter\n",
    "    'lambda_sparse': (0.0001, 0.01),  # Sparsity regularization\n",
    "    'lr': (0.005, 0.05),      # Learning rate\n",
    "    'momentum': (0.01, 0.3),  # Momentum for batch normalization\n",
    "    'batch_size': (128, 1024), # Batch size (must be power of 2 or multiple of 64)\n",
    "    'n_independent': (1, 4),  # Number of independent GLU layers\n",
    "    'n_shared': (1, 4),       # Number of shared GLU layers\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION CLASSES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TabNetConfig:\n",
    "    \"\"\"Configuration for TabNet model and training with validation.\"\"\"\n",
    "    \n",
    "    # Model architecture\n",
    "    n_d: int = 16\n",
    "    n_a: int = 16\n",
    "    n_steps: int = 5\n",
    "    gamma: float = 1.5\n",
    "    lambda_sparse: float = 0.001\n",
    "    lr: float = 0.02\n",
    "    momentum: float = 0.02\n",
    "    n_independent: int = 2\n",
    "    n_shared: int = 2\n",
    "    \n",
    "    # Training parameters\n",
    "    max_epochs: int = 50\n",
    "    patience: int = 10\n",
    "    batch_size: int = 512\n",
    "    virtual_batch_size: int = 64\n",
    "    \n",
    "    # Cross-validation parameters\n",
    "    use_cross_validation: bool = True\n",
    "    n_folds: int = 5\n",
    "    cv_random_state: int = 42\n",
    "    \n",
    "    # Genetic Algorithm parameters\n",
    "    use_ga_optimization: bool = True\n",
    "    ga_population_size: int = 20\n",
    "    ga_generations: int = 15\n",
    "    ga_mutation_rate: float = 0.3\n",
    "    ga_crossover_rate: float = 0.7\n",
    "    ga_tournament_size: int = 3\n",
    "    \n",
    "    # Visualization\n",
    "    display_plots: bool = True\n",
    "    save_plots: bool = True\n",
    "    plot_dpi: int = 100\n",
    "    block_on_plot: bool = False\n",
    "    \n",
    "    # Environment\n",
    "    random_seed: int = 42\n",
    "    deterministic: bool = True\n",
    "    \n",
    "    # Paths\n",
    "    dataset_path: str = \"data.csv\"\n",
    "    results_dir: str = \"tabnet_results\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration parameters.\"\"\"\n",
    "        self._validate()\n",
    "    \n",
    "    def _validate(self):\n",
    "        \"\"\"Validate all configuration parameters.\"\"\"\n",
    "        if self.n_d <= 0 or self.n_a <= 0:\n",
    "            raise ValueError(f\"n_d and n_a must be positive (got n_d={self.n_d}, n_a={self.n_a})\")\n",
    "        \n",
    "        if self.n_steps <= 0:\n",
    "            raise ValueError(f\"n_steps must be positive (got {self.n_steps})\")\n",
    "        \n",
    "        if not 0 <= self.gamma <= 10:\n",
    "            raise ValueError(f\"gamma should be in [0, 10] (got {self.gamma})\")\n",
    "        \n",
    "        if not 0 <= self.lambda_sparse <= 1:\n",
    "            raise ValueError(f\"lambda_sparse should be in [0, 1] (got {self.lambda_sparse})\")\n",
    "        \n",
    "        if self.lr <= 0:\n",
    "            raise ValueError(f\"lr must be positive (got {self.lr})\")\n",
    "        \n",
    "        if self.max_epochs <= 0:\n",
    "            raise ValueError(f\"max_epochs must be positive (got {self.max_epochs})\")\n",
    "        \n",
    "        if self.patience <= 0:\n",
    "            raise ValueError(f\"patience must be positive (got {self.patience})\")\n",
    "        \n",
    "        if self.batch_size <= 0 or self.virtual_batch_size <= 0:\n",
    "            raise ValueError(\"batch_size and virtual_batch_size must be positive\")\n",
    "        \n",
    "        if self.virtual_batch_size > self.batch_size:\n",
    "            raise ValueError(\"virtual_batch_size must be <= batch_size\")\n",
    "        \n",
    "        if self.use_cross_validation and self.n_folds < 2:\n",
    "            raise ValueError(f\"n_folds must be >= 2 (got {self.n_folds})\")\n",
    "        \n",
    "        if not Path(self.dataset_path).exists():\n",
    "            raise FileNotFoundError(f\"Dataset not found: {self.dataset_path}\")\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Configuration for data loading and preprocessing with validation.\"\"\"\n",
    "    \n",
    "    target_column: Optional[str] = None\n",
    "    drop_columns: List[str] = field(default_factory=lambda: DEFAULT_DROP_COLUMNS.copy())\n",
    "    test_size: float = 0.2\n",
    "    validation_size: float = 0.1\n",
    "    scale_features: bool = True\n",
    "    handle_missing: str = \"mean\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self._validate()\n",
    "    \n",
    "    def _validate(self):\n",
    "        if not 0 < self.test_size < 1:\n",
    "            raise ValueError(f\"test_size must be in (0, 1) (got {self.test_size})\")\n",
    "        \n",
    "        if not 0 < self.validation_size < 1:\n",
    "            raise ValueError(f\"validation_size must be in (0, 1) (got {self.validation_size})\")\n",
    "        \n",
    "        if self.test_size + self.validation_size >= 1:\n",
    "            raise ValueError(\"test_size + validation_size must be < 1\")\n",
    "        \n",
    "        if self.handle_missing not in [\"mean\", \"median\", \"drop\"]:\n",
    "            raise ValueError(f\"handle_missing must be 'mean', 'median', or 'drop' (got {self.handle_missing})\")\n",
    "\n",
    "# =============================================================================\n",
    "# CROSS-VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class CVResults:\n",
    "    \"\"\"Stores cross-validation results.\"\"\"\n",
    "    fold_metrics: List[Dict[str, float]] = field(default_factory=list)\n",
    "    fold_histories: List[Dict] = field(default_factory=list)\n",
    "    best_fold: int = -1\n",
    "    mean_metrics: Dict[str, float] = field(default_factory=dict)\n",
    "    std_metrics: Dict[str, float] = field(default_factory=dict)\n",
    "    \n",
    "    def add_fold_result(self, fold_idx: int, metrics: Dict, history: Dict):\n",
    "        \"\"\"Add results from a single fold.\"\"\"\n",
    "        self.fold_metrics.append(metrics)\n",
    "        self.fold_histories.append(history)\n",
    "    \n",
    "    def compute_statistics(self):\n",
    "        \"\"\"Compute mean and std across folds.\"\"\"\n",
    "        if not self.fold_metrics:\n",
    "            return\n",
    "        \n",
    "        metric_keys = ['accuracy', 'auc', 'sensitivity', 'specificity', 'precision', 'f1', 'npv']\n",
    "        \n",
    "        for key in metric_keys:\n",
    "            values = [fold[key] for fold in self.fold_metrics if key in fold]\n",
    "            if values:\n",
    "                self.mean_metrics[key] = np.mean(values)\n",
    "                self.std_metrics[key] = np.std(values)\n",
    "        \n",
    "        if 'auc' in self.mean_metrics:\n",
    "            auc_scores = [fold.get('auc', 0) for fold in self.fold_metrics]\n",
    "            self.best_fold = int(np.argmax(auc_scores))\n",
    "    \n",
    "    def get_overfitting_analysis(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze overfitting by comparing training and validation performance.\"\"\"\n",
    "        analysis = {\n",
    "            'variance': {},\n",
    "            'consistency': {},\n",
    "            'overfitting_indicators': []\n",
    "        }\n",
    "        \n",
    "        for metric, mean_val in self.mean_metrics.items():\n",
    "            if metric in self.std_metrics and mean_val > 0:\n",
    "                cv = self.std_metrics[metric] / mean_val\n",
    "                analysis['variance'][metric] = cv\n",
    "                \n",
    "                if cv > 0.1:\n",
    "                    analysis['overfitting_indicators'].append(\n",
    "                        f\"High variance in {metric}: CV={cv:.3f}\"\n",
    "                    )\n",
    "        \n",
    "        for metric in ['accuracy', 'auc', 'f1']:\n",
    "            if metric in self.std_metrics:\n",
    "                std = self.std_metrics[metric]\n",
    "                analysis['consistency'][metric] = 'Good' if std < 0.05 else 'Moderate' if std < 0.10 else 'Poor'\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "class Logger:\n",
    "    \"\"\"Enhanced logger with context management.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, level: int = logging.INFO):\n",
    "        self.logger = logging.getLogger(name)\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler(sys.stdout)\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    "                datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "            self.logger.setLevel(level)\n",
    "    \n",
    "    @contextmanager\n",
    "    def log_section(self, title: str):\n",
    "        \"\"\"Context manager for logging sections.\"\"\"\n",
    "        self.logger.info(\"=\" * 70)\n",
    "        self.logger.info(f\" {title}\")\n",
    "        self.logger.info(\"=\" * 70)\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            self.logger.info(\"\")\n",
    "    \n",
    "    def info(self, msg: str):\n",
    "        self.logger.info(msg)\n",
    "    \n",
    "    def warning(self, msg: str):\n",
    "        self.logger.warning(msg)\n",
    "    \n",
    "    def error(self, msg: str):\n",
    "        self.logger.error(msg)\n",
    "    \n",
    "    def debug(self, msg: str):\n",
    "        self.logger.debug(msg)\n",
    "\n",
    "@contextmanager\n",
    "def timer(logger: Logger, operation: str):\n",
    "    \"\"\"Context manager for timing operations.\"\"\"\n",
    "    start_time = datetime.now()\n",
    "    logger.info(f\"Starting: {operation}\")\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        logger.info(f\"âœ“ Completed: {operation} ({elapsed:.2f}s)\")\n",
    "\n",
    "def set_random_seeds(seed: int):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    import random\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Aggressively clear memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            torch.cuda.synchronize()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PROCESSING CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Handles all data loading and preprocessing operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataConfig, logger: Logger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.scaler = None\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def load_dataset(self, filepath: Path) -> pd.DataFrame:\n",
    "        \"\"\"Load dataset from file with error handling.\"\"\"\n",
    "        if not filepath.exists():\n",
    "            raise FileNotFoundError(f\"Dataset not found: {filepath}\")\n",
    "        \n",
    "        try:\n",
    "            with timer(self.logger, f\"Loading dataset from {filepath}\"):\n",
    "                df = pd.read_csv(filepath)\n",
    "                self.logger.info(f\"   Shape: {df.shape}\")\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load dataset: {e}\")\n",
    "    \n",
    "    def identify_target_column(self, df: pd.DataFrame) -> str:\n",
    "        \"\"\"Identify the target column in the dataset.\"\"\"\n",
    "        if self.config.target_column is not None:\n",
    "            if self.config.target_column in df.columns:\n",
    "                return self.config.target_column\n",
    "            else:\n",
    "                raise ValueError(f\"Specified target column '{self.config.target_column}' not found\")\n",
    "        \n",
    "        for candidate in TARGET_CANDIDATES:\n",
    "            if candidate in df.columns:\n",
    "                self.logger.info(f\"Auto-detected target column: '{candidate}'\")\n",
    "                return candidate\n",
    "        \n",
    "        target_col = df.columns[-1]\n",
    "        self.logger.warning(f\"Using last column as target: '{target_col}'\")\n",
    "        return target_col\n",
    "    \n",
    "    def preprocess_target(self, target_series: pd.Series) -> np.ndarray:\n",
    "        \"\"\"Convert target to binary format with robust handling.\"\"\"\n",
    "        target_series = target_series.copy()\n",
    "        \n",
    "        if target_series.dtype == 'object' or target_series.dtype.name == 'category':\n",
    "            return self._preprocess_categorical_target(target_series)\n",
    "        \n",
    "        return self._preprocess_numeric_target(target_series)\n",
    "    \n",
    "    def _preprocess_categorical_target(self, target_series: pd.Series) -> np.ndarray:\n",
    "        \"\"\"Preprocess categorical target values.\"\"\"\n",
    "        normalized = target_series.astype(str).str.strip().str.lower()\n",
    "        \n",
    "        mapping = {\n",
    "            'm': 1, 'malignant': 1, '1': 1, '4': 1, 'positive': 1, 'yes': 1,\n",
    "            'b': 0, 'benign': 0, '0': 0, '2': 0, 'negative': 0, 'no': 0\n",
    "        }\n",
    "        \n",
    "        mapped = normalized.map(mapping)\n",
    "        \n",
    "        if not mapped.isna().any():\n",
    "            return mapped.astype(int).values\n",
    "        \n",
    "        numeric_vals = pd.to_numeric(normalized, errors='coerce')\n",
    "        if numeric_vals.notna().all():\n",
    "            return self._preprocess_numeric_target(numeric_vals)\n",
    "        \n",
    "        self.logger.warning(\"Using LabelEncoder for target conversion\")\n",
    "        encoder = LabelEncoder()\n",
    "        encoded = encoder.fit_transform(normalized)\n",
    "        \n",
    "        if len(encoder.classes_) != 2:\n",
    "            raise ValueError(f\"Expected binary target, got {len(encoder.classes_)} classes\")\n",
    "        \n",
    "        return encoded.astype(int)\n",
    "    \n",
    "    def _preprocess_numeric_target(self, target_series: pd.Series) -> np.ndarray:\n",
    "        \"\"\"Preprocess numeric target values.\"\"\"\n",
    "        numeric_vals = pd.to_numeric(target_series, errors='coerce')\n",
    "        \n",
    "        if numeric_vals.isna().any():\n",
    "            raise ValueError(\"Target contains non-numeric values that cannot be converted\")\n",
    "        \n",
    "        unique_vals = set(numeric_vals.dropna().unique())\n",
    "        \n",
    "        if unique_vals.issubset({0, 1}):\n",
    "            return numeric_vals.astype(int).values\n",
    "        elif unique_vals.issubset({2, 4}):\n",
    "            return (numeric_vals == 4).astype(int).values\n",
    "        elif len(unique_vals) == 2:\n",
    "            sorted_vals = sorted(unique_vals)\n",
    "            return (numeric_vals == sorted_vals[1]).astype(int).values\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot interpret target values: {unique_vals}\")\n",
    "    \n",
    "    def prepare_features(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
    "        \"\"\"Prepare feature matrix by dropping unnecessary columns.\"\"\"\n",
    "        drop_cols = [col for col in self.config.drop_columns \n",
    "                    if col in df.columns and col != target_col]\n",
    "        \n",
    "        if target_col not in df.columns:\n",
    "            raise ValueError(f\"Target column '{target_col}' not found in dataframe\")\n",
    "        \n",
    "        X_df = df.drop(columns=[target_col] + drop_cols, errors='ignore')\n",
    "        \n",
    "        for col in X_df.columns:\n",
    "            X_df[col] = pd.to_numeric(X_df[col], errors='coerce')\n",
    "        \n",
    "        X_df = X_df.dropna(axis=1, how='all')\n",
    "        \n",
    "        if X_df.shape[1] == 0:\n",
    "            raise ValueError(\"No valid features remaining after preprocessing\")\n",
    "        \n",
    "        self.feature_names = list(X_df.columns)\n",
    "        return X_df\n",
    "    \n",
    "    def handle_missing_values(self, X_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Handle missing values according to configuration.\"\"\"\n",
    "        missing_cols = X_df.columns[X_df.isna().any()].tolist()\n",
    "        \n",
    "        if not missing_cols:\n",
    "            return X_df\n",
    "        \n",
    "        self.logger.warning(f\"âš  Missing values detected in {len(missing_cols)} columns\")\n",
    "        \n",
    "        if self.config.handle_missing == \"mean\":\n",
    "            X_df = X_df.fillna(X_df.mean())\n",
    "        elif self.config.handle_missing == \"median\":\n",
    "            X_df = X_df.fillna(X_df.median())\n",
    "        elif self.config.handle_missing == \"drop\":\n",
    "            X_df = X_df.dropna()\n",
    "        \n",
    "        return X_df\n",
    "    \n",
    "    def split_data(self, X: np.ndarray, y: np.ndarray, \n",
    "                   random_state: int) -> Tuple[np.ndarray, ...]:\n",
    "        \"\"\"Split data into train, validation, and test sets.\"\"\"\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, test_size=self.config.test_size, \n",
    "            stratify=y, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        val_size = self.config.validation_size / (1 - self.config.test_size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size,\n",
    "            stratify=y_temp, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"   Train set: {X_train.shape[0]} samples\")\n",
    "        self.logger.info(f\"   Validation set: {X_val.shape[0]} samples\")\n",
    "        self.logger.info(f\"   Test set: {X_test.shape[0]} samples\")\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \n",
    "    def scale_features(self, X_train: np.ndarray, X_val: np.ndarray, \n",
    "                      X_test: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Scale features using StandardScaler.\"\"\"\n",
    "        if not self.config.scale_features:\n",
    "            return X_train, X_val, X_test\n",
    "        \n",
    "        with timer(self.logger, \"Feature scaling\"):\n",
    "            self.scaler = StandardScaler()\n",
    "            X_train = self.scaler.fit_transform(X_train).astype(np.float32)\n",
    "            X_val = self.scaler.transform(X_val).astype(np.float32)\n",
    "            X_test = self.scaler.transform(X_test).astype(np.float32)\n",
    "        \n",
    "        return X_train, X_val, X_test\n",
    "    \n",
    "    def load_and_preprocess(self, filepath: Path, random_state: int) -> Tuple[np.ndarray, ...]:\n",
    "        \"\"\"Complete data loading and preprocessing pipeline.\"\"\"\n",
    "        df = self.load_dataset(filepath)\n",
    "        target_col = self.identify_target_column(df)\n",
    "        X_df = self.prepare_features(df, target_col)\n",
    "        X_df = self.handle_missing_values(X_df)\n",
    "        y = self.preprocess_target(df[target_col])\n",
    "        \n",
    "        if X_df.shape[0] != len(y):\n",
    "            raise ValueError(f\"Shape mismatch: X has {X_df.shape[0]} samples but y has {len(y)}\")\n",
    "        \n",
    "        benign_count = np.sum(y == 0)\n",
    "        malignant_count = np.sum(y == 1)\n",
    "        self.logger.info(f\"   Class distribution: Benign={benign_count} ({benign_count/len(y)*100:.1f}%), \"\n",
    "                        f\"Malignant={malignant_count} ({malignant_count/len(y)*100:.1f}%)\")\n",
    "        self.logger.info(f\"   Features: {len(self.feature_names)}\")\n",
    "        \n",
    "        X = X_df.values.astype(np.float32)\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = self.split_data(X, y, random_state)\n",
    "        X_train, X_val, X_test = self.scale_features(X_train, X_val, X_test)\n",
    "        \n",
    "        clear_memory()\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# =============================================================================\n",
    "# METRICS COMPUTATION \n",
    "# =============================================================================\n",
    "\n",
    "class MetricsComputer:\n",
    "    \"\"\"Computes and formats model evaluation metrics.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_all_metrics(y_true: np.ndarray, y_pred: np.ndarray, \n",
    "                           y_proba: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Compute comprehensive evaluation metrics.\"\"\"\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        auc = roc_auc_score(y_true, y_proba)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "        f1 = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0.0\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"auc\": float(auc),\n",
    "            \"sensitivity\": float(sensitivity),\n",
    "            \"specificity\": float(specificity),\n",
    "            \"precision\": float(precision),\n",
    "            \"npv\": float(npv),\n",
    "            \"f1\": float(f1),\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"roc_curve\": (fpr, tpr),\n",
    "            \"tn\": int(tn), \n",
    "            \"fp\": int(fp), \n",
    "            \"fn\": int(fn), \n",
    "            \"tp\": int(tp)\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_metrics_summary(metrics: Dict[str, Any], prefix: str = \"\") -> str:\n",
    "        \"\"\"Format metrics as a readable summary.\"\"\"\n",
    "        lines = [\n",
    "            f\"\\n {prefix}MODEL PERFORMANCE METRICS:\",\n",
    "            f\"   Accuracy:    {metrics['accuracy']:.4f}\",\n",
    "            f\"   AUC-ROC:     {metrics['auc']:.4f}\",\n",
    "            f\"   Sensitivity: {metrics['sensitivity']:.4f}\",\n",
    "            f\"   Specificity: {metrics['specificity']:.4f}\",\n",
    "            f\"   Precision:   {metrics['precision']:.4f}\",\n",
    "            f\"   NPV:         {metrics['npv']:.4f}\",\n",
    "            f\"   F1 Score:    {metrics['f1']:.4f}\",\n",
    "            \"\\n CONFUSION MATRIX:\",\n",
    "            f\"   TN: {metrics['tn']:4d}  FP: {metrics['fp']:4d}\",\n",
    "            f\"   FN: {metrics['fn']:4d}  TP: {metrics['tp']:4d}\"\n",
    "        ]\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# =============================================================================\n",
    "# GENETIC ALGORITHM FOR HYPERPARAMETER OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "class GeneticOptimizer:\n",
    "    \"\"\"Genetic algorithm optimizer for TabNet hyperparameters using DEAP.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TabNetConfig, data_config: DataConfig, \n",
    "                 X_train: np.ndarray, X_val: np.ndarray,\n",
    "                 y_train: np.ndarray, y_val: np.ndarray, logger: Logger):\n",
    "        self.config = config\n",
    "        self.data_config = data_config\n",
    "        self.X_train = X_train\n",
    "        self.X_val = X_val\n",
    "        self.y_train = y_train\n",
    "        self.y_val = y_val\n",
    "        self.logger = logger\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Evolution tracking\n",
    "        self.generation_stats = []\n",
    "        self.best_individuals = []\n",
    "        self.evaluation_cache = {}\n",
    "        \n",
    "        # Setup DEAP\n",
    "        self._setup_deap()\n",
    "    \n",
    "    def _setup_deap(self):\n",
    "        \"\"\"Setup DEAP framework for genetic algorithm.\"\"\"\n",
    "        # Create fitness and individual classes\n",
    "        if hasattr(creator, \"FitnessMax\"):\n",
    "            del creator.FitnessMax\n",
    "        if hasattr(creator, \"Individual\"):\n",
    "            del creator.Individual\n",
    "            \n",
    "        creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # Maximize fitness\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "        \n",
    "        # Create toolbox\n",
    "        self.toolbox = base.Toolbox()\n",
    "        \n",
    "        # Register attribute generators\n",
    "        self.toolbox.register(\"attr_n_d\", random.randint, *HYPERPARAMETER_BOUNDS['n_d'])\n",
    "        self.toolbox.register(\"attr_n_a\", random.randint, *HYPERPARAMETER_BOUNDS['n_a'])\n",
    "        self.toolbox.register(\"attr_n_steps\", random.randint, *HYPERPARAMETER_BOUNDS['n_steps'])\n",
    "        self.toolbox.register(\"attr_gamma\", random.uniform, *HYPERPARAMETER_BOUNDS['gamma'])\n",
    "        self.toolbox.register(\"attr_lambda_sparse\", random.uniform, *HYPERPARAMETER_BOUNDS['lambda_sparse'])\n",
    "        self.toolbox.register(\"attr_lr\", random.uniform, *HYPERPARAMETER_BOUNDS['lr'])\n",
    "        self.toolbox.register(\"attr_momentum\", random.uniform, *HYPERPARAMETER_BOUNDS['momentum'])\n",
    "        self.toolbox.register(\"attr_batch_size\", self._generate_batch_size)\n",
    "        self.toolbox.register(\"attr_n_independent\", random.randint, *HYPERPARAMETER_BOUNDS['n_independent'])\n",
    "        self.toolbox.register(\"attr_n_shared\", random.randint, *HYPERPARAMETER_BOUNDS['n_shared'])\n",
    "        \n",
    "        # Register individual and population\n",
    "        self.toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                             (self.toolbox.attr_n_d, self.toolbox.attr_n_a, \n",
    "                              self.toolbox.attr_n_steps, self.toolbox.attr_gamma,\n",
    "                              self.toolbox.attr_lambda_sparse, self.toolbox.attr_lr,\n",
    "                              self.toolbox.attr_momentum, self.toolbox.attr_batch_size,\n",
    "                              self.toolbox.attr_n_independent, self.toolbox.attr_n_shared), n=1)\n",
    "        \n",
    "        self.toolbox.register(\"population\", tools.initRepeat, list, self.toolbox.individual)\n",
    "        \n",
    "        # Register genetic operators\n",
    "        self.toolbox.register(\"evaluate\", self._evaluate_individual)\n",
    "        self.toolbox.register(\"mate\", self._crossover)\n",
    "        self.toolbox.register(\"mutate\", self._mutate)\n",
    "        self.toolbox.register(\"select\", tools.selTournament, tournsize=self.config.ga_tournament_size)\n",
    "        \n",
    "        self.logger.info(\"DEAP genetic algorithm framework initialized\")\n",
    "    \n",
    "    def _generate_batch_size(self) -> int:\n",
    "        \"\"\"Generate valid batch size (power of 2 or multiple of 64).\"\"\"\n",
    "        min_bs, max_bs = HYPERPARAMETER_BOUNDS['batch_size']\n",
    "        # Generate multiples of 64\n",
    "        multiples = [64 * i for i in range(min_bs // 64, (max_bs // 64) + 1)]\n",
    "        return random.choice(multiples)\n",
    "    \n",
    "    def _individual_to_hyperparams(self, individual: List) -> Dict:\n",
    "        \"\"\"Convert DEAP individual to hyperparameter dictionary.\"\"\"\n",
    "        return {\n",
    "            'n_d': int(individual[0]),\n",
    "            'n_a': int(individual[1]),\n",
    "            'n_steps': int(individual[2]),\n",
    "            'gamma': float(individual[3]),\n",
    "            'lambda_sparse': float(individual[4]),\n",
    "            'lr': float(individual[5]),\n",
    "            'momentum': float(individual[6]),\n",
    "            'batch_size': int(individual[7]),\n",
    "            'n_independent': int(individual[8]),\n",
    "            'n_shared': int(individual[9])\n",
    "        }\n",
    "    \n",
    "    def _hyperparams_to_tuple(self, hyperparams: Dict) -> Tuple:\n",
    "        \"\"\"Convert hyperparameters to hashable tuple for caching.\"\"\"\n",
    "        return tuple(sorted(hyperparams.items()))\n",
    "    \n",
    "    def _evaluate_individual(self, individual: List) -> Tuple[float]:\n",
    "        \"\"\"Evaluate fitness of an individual (hyperparameter set).\"\"\"\n",
    "        hyperparams = self._individual_to_hyperparams(individual)\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = self._hyperparams_to_tuple(hyperparams)\n",
    "        if cache_key in self.evaluation_cache:\n",
    "            return self.evaluation_cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            # Create TabNet model with these hyperparameters\n",
    "            virtual_batch_size = min(hyperparams['batch_size'] // 8, 128)\n",
    "            \n",
    "            model = TabNetClassifier(\n",
    "                n_d=hyperparams['n_d'],\n",
    "                n_a=hyperparams['n_a'],\n",
    "                n_steps=hyperparams['n_steps'],\n",
    "                gamma=hyperparams['gamma'],\n",
    "                lambda_sparse=hyperparams['lambda_sparse'],\n",
    "                momentum=hyperparams['momentum'],\n",
    "                n_independent=hyperparams['n_independent'],\n",
    "                n_shared=hyperparams['n_shared'],\n",
    "                optimizer_fn=torch.optim.Adam,\n",
    "                optimizer_params={\"lr\": hyperparams['lr'], \"weight_decay\": 1e-5},\n",
    "                scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                scheduler_params={\"step_size\": 10, \"gamma\": 0.9},\n",
    "                device_name=str(self.device),\n",
    "                verbose=0,\n",
    "                seed=self.config.random_seed\n",
    "            )\n",
    "            \n",
    "            # Train model with early stopping\n",
    "            model.fit(\n",
    "                self.X_train, self.y_train,\n",
    "                eval_set=[(self.X_val, self.y_val)],\n",
    "                eval_name=[\"val\"],\n",
    "                eval_metric=[\"auc\"],\n",
    "                max_epochs=min(30, self.config.max_epochs),  # Reduced epochs for GA\n",
    "                patience=8,\n",
    "                batch_size=hyperparams['batch_size'],\n",
    "                virtual_batch_size=virtual_batch_size,\n",
    "                drop_last=False,\n",
    "                num_workers=0,\n",
    "            )\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            y_pred = model.predict(self.X_val)\n",
    "            y_proba = model.predict_proba(self.X_val)[:, 1]\n",
    "            \n",
    "            # Compute fitness as weighted combination of metrics\n",
    "            accuracy = accuracy_score(self.y_val, y_pred)\n",
    "            auc = roc_auc_score(self.y_val, y_proba)\n",
    "            \n",
    "            cm = confusion_matrix(self.y_val, y_pred)\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "            f1 = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0.0\n",
    "            \n",
    "            # Weighted fitness function emphasizing AUC and F1\n",
    "            fitness = (0.30 * auc + 0.25 * f1 + 0.20 * accuracy + \n",
    "                      0.15 * sensitivity + 0.10 * specificity)\n",
    "            \n",
    "            # Cache result\n",
    "            self.evaluation_cache[cache_key] = (fitness,)\n",
    "            \n",
    "            # Clear memory\n",
    "            del model\n",
    "            clear_memory()\n",
    "            \n",
    "            return (fitness,)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"   Evaluation failed for individual: {e}\")\n",
    "            return (0.0,)  # Return poor fitness for failed evaluations\n",
    "    \n",
    "    def _crossover(self, ind1: List, ind2: List) -> Tuple[List, List]:\n",
    "        \"\"\"Custom crossover operation for hyperparameters.\"\"\"\n",
    "        # Blend crossover for continuous variables, uniform for discrete\n",
    "        offspring1 = ind1[:]\n",
    "        offspring2 = ind2[:]\n",
    "        \n",
    "        alpha = 0.5  # Blend factor\n",
    "        \n",
    "        for i in range(len(ind1)):\n",
    "            if random.random() < 0.5:\n",
    "                if i in [0, 1, 2, 7, 8, 9]:  # Discrete parameters\n",
    "                    offspring1[i], offspring2[i] = ind2[i], ind1[i]\n",
    "                else:  # Continuous parameters\n",
    "                    offspring1[i] = alpha * ind1[i] + (1 - alpha) * ind2[i]\n",
    "                    offspring2[i] = alpha * ind2[i] + (1 - alpha) * ind1[i]\n",
    "        \n",
    "        return offspring1, offspring2\n",
    "    \n",
    "    def _mutate(self, individual: List) -> Tuple[List]:\n",
    "        \"\"\"Custom mutation operation for hyperparameters.\"\"\"\n",
    "        mutated = individual[:]\n",
    "        \n",
    "        for i in range(len(mutated)):\n",
    "            if random.random() < self.config.ga_mutation_rate:\n",
    "                if i == 0:  # n_d\n",
    "                    mutated[i] = random.randint(*HYPERPARAMETER_BOUNDS['n_d'])\n",
    "                elif i == 1:  # n_a\n",
    "                    mutated[i] = random.randint(*HYPERPARAMETER_BOUNDS['n_a'])\n",
    "                elif i == 2:  # n_steps\n",
    "                    mutated[i] = random.randint(*HYPERPARAMETER_BOUNDS['n_steps'])\n",
    "                elif i == 3:  # gamma\n",
    "                    mutated[i] = random.uniform(*HYPERPARAMETER_BOUNDS['gamma'])\n",
    "                elif i == 4:  # lambda_sparse\n",
    "                    mutated[i] = random.uniform(*HYPERPARAMETER_BOUNDS['lambda_sparse'])\n",
    "                elif i == 5:  # lr\n",
    "                    mutated[i] = random.uniform(*HYPERPARAMETER_BOUNDS['lr'])\n",
    "                elif i == 6:  # momentum\n",
    "                    mutated[i] = random.uniform(*HYPERPARAMETER_BOUNDS['momentum'])\n",
    "                elif i == 7:  # batch_size\n",
    "                    mutated[i] = self._generate_batch_size()\n",
    "                elif i == 8:  # n_independent\n",
    "                    mutated[i] = random.randint(*HYPERPARAMETER_BOUNDS['n_independent'])\n",
    "                elif i == 9:  # n_shared\n",
    "                    mutated[i] = random.randint(*HYPERPARAMETER_BOUNDS['n_shared'])\n",
    "        \n",
    "        return (mutated,)\n",
    "    \n",
    "    def optimize(self) -> Dict:\n",
    "        \"\"\"Run genetic algorithm optimization.\"\"\"\n",
    "        with self.logger.log_section(\"ðŸ§¬ GENETIC ALGORITHM OPTIMIZATION\"):\n",
    "            self.logger.info(f\"Population Size: {self.config.ga_population_size}\")\n",
    "            self.logger.info(f\"Generations: {self.config.ga_generations}\")\n",
    "            self.logger.info(f\"Mutation Rate: {self.config.ga_mutation_rate}\")\n",
    "            self.logger.info(f\"Crossover Rate: {self.config.ga_crossover_rate}\")\n",
    "            \n",
    "            # Create initial population\n",
    "            population = self.toolbox.population(n=self.config.ga_population_size)\n",
    "            \n",
    "            # Statistics\n",
    "            stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "            stats.register(\"avg\", np.mean)\n",
    "            stats.register(\"std\", np.std)\n",
    "            stats.register(\"min\", np.min)\n",
    "            stats.register(\"max\", np.max)\n",
    "            \n",
    "            # Hall of fame\n",
    "            hof = tools.HallOfFame(5)\n",
    "            \n",
    "            # Run evolution\n",
    "            self.logger.info(\"\\nðŸ”„ Starting evolution...\")\n",
    "            \n",
    "            for gen in range(self.config.ga_generations):\n",
    "                self.logger.info(f\"\\n{'='*60}\")\n",
    "                self.logger.info(f\"  GENERATION {gen + 1}/{self.config.ga_generations}\")\n",
    "                self.logger.info(f\"{'='*60}\")\n",
    "                \n",
    "                # Evaluate population\n",
    "                fitnesses = list(map(self.toolbox.evaluate, population))\n",
    "                for ind, fit in zip(population, fitnesses):\n",
    "                    ind.fitness.values = fit\n",
    "                \n",
    "                # Update hall of fame\n",
    "                hof.update(population)\n",
    "                \n",
    "                # Record statistics\n",
    "                record = stats.compile(population)\n",
    "                self.generation_stats.append(record)\n",
    "                \n",
    "                self.logger.info(f\"   Fitness - Max: {record['max']:.6f}, \"\n",
    "                               f\"Avg: {record['avg']:.6f}, \"\n",
    "                               f\"Min: {record['min']:.6f}, \"\n",
    "                               f\"Std: {record['std']:.6f}\")\n",
    "                \n",
    "                # Store best individual of this generation\n",
    "                best_ind = tools.selBest(population, 1)[0]\n",
    "                self.best_individuals.append((gen, best_ind[:], best_ind.fitness.values[0]))\n",
    "                \n",
    "                # Selection\n",
    "                offspring = self.toolbox.select(population, len(population))\n",
    "                offspring = list(map(self.toolbox.clone, offspring))\n",
    "                \n",
    "                # Crossover\n",
    "                for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "                    if random.random() < self.config.ga_crossover_rate:\n",
    "                        self.toolbox.mate(child1, child2)\n",
    "                        del child1.fitness.values\n",
    "                        del child2.fitness.values\n",
    "                \n",
    "                # Mutation\n",
    "                for mutant in offspring:\n",
    "                    if random.random() < self.config.ga_mutation_rate:\n",
    "                        self.toolbox.mutate(mutant)\n",
    "                        del mutant.fitness.values\n",
    "                \n",
    "                # Evaluate offspring with invalid fitness\n",
    "                invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "                fitnesses = map(self.toolbox.evaluate, invalid_ind)\n",
    "                for ind, fit in zip(invalid_ind, fitnesses):\n",
    "                    ind.fitness.values = fit\n",
    "                \n",
    "                # Replace population\n",
    "                population[:] = offspring\n",
    "                \n",
    "                # Clear some memory periodically\n",
    "                if (gen + 1) % 5 == 0:\n",
    "                    clear_memory()\n",
    "            \n",
    "            # Get best hyperparameters\n",
    "            best_individual = hof[0]\n",
    "            best_hyperparams = self._individual_to_hyperparams(best_individual)\n",
    "            best_fitness = best_individual.fitness.values[0]\n",
    "            \n",
    "            self.logger.info(\"\\n\" + \"=\"*70)\n",
    "            self.logger.info(\"   OPTIMIZATION COMPLETE\")\n",
    "            self.logger.info(\"=\"*70)\n",
    "            self.logger.info(f\"\\n Best Fitness Achieved: {best_fitness:.6f}\")\n",
    "            self.logger.info(f\"\\n Optimal Hyperparameters:\")\n",
    "            for param, value in best_hyperparams.items():\n",
    "                self.logger.info(f\"   {param:15s}: {value}\")\n",
    "            \n",
    "            return {\n",
    "                'best_hyperparams': best_hyperparams,\n",
    "                'best_fitness': best_fitness,\n",
    "                'best_individual': best_individual,\n",
    "                'hall_of_fame': [self._individual_to_hyperparams(ind) for ind in hof],\n",
    "                'generation_stats': self.generation_stats,\n",
    "                'evolution_history': self.best_individuals\n",
    "            }\n",
    "    \n",
    "    def plot_evolution_progress(self, save_dir: Path):\n",
    "        \"\"\"Plot evolution progress over generations.\"\"\"\n",
    "        if not self.generation_stats:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "            fig.suptitle('Genetic Algorithm Evolution Progress', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            generations = list(range(1, len(self.generation_stats) + 1))\n",
    "            \n",
    "            # Max fitness\n",
    "            ax = axes[0, 0]\n",
    "            max_fitness = [stat['max'] for stat in self.generation_stats]\n",
    "            ax.plot(generations, max_fitness, 'o-', linewidth=2, markersize=6, color='green', label='Max Fitness')\n",
    "            ax.set_xlabel('Generation', fontsize=11, fontweight='bold')\n",
    "            ax.set_ylabel('Maximum Fitness', fontsize=11, fontweight='bold')\n",
    "            ax.set_title('Best Fitness per Generation', fontsize=12, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "            \n",
    "            # Average fitness\n",
    "            ax = axes[0, 1]\n",
    "            avg_fitness = [stat['avg'] for stat in self.generation_stats]\n",
    "            std_fitness = [stat['std'] for stat in self.generation_stats]\n",
    "            ax.plot(generations, avg_fitness, 'o-', linewidth=2, markersize=6, color='blue', label='Avg Fitness')\n",
    "            ax.fill_between(generations, \n",
    "                           np.array(avg_fitness) - np.array(std_fitness),\n",
    "                           np.array(avg_fitness) + np.array(std_fitness),\n",
    "                           alpha=0.2, color='blue')\n",
    "            ax.set_xlabel('Generation', fontsize=11, fontweight='bold')\n",
    "            ax.set_ylabel('Average Fitness', fontsize=11, fontweight='bold')\n",
    "            ax.set_title('Average Fitness with Â±1 Std Dev', fontsize=12, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "            \n",
    "            # Fitness range\n",
    "            ax = axes[1, 0]\n",
    "            min_fitness = [stat['min'] for stat in self.generation_stats]\n",
    "            ax.fill_between(generations, min_fitness, max_fitness, alpha=0.3, color='purple')\n",
    "            ax.plot(generations, max_fitness, 'o-', linewidth=2, markersize=4, color='green', label='Max')\n",
    "            ax.plot(generations, avg_fitness, 's-', linewidth=2, markersize=4, color='blue', label='Avg')\n",
    "            ax.plot(generations, min_fitness, '^-', linewidth=2, markersize=4, color='red', label='Min')\n",
    "            ax.set_xlabel('Generation', fontsize=11, fontweight='bold')\n",
    "            ax.set_ylabel('Fitness', fontsize=11, fontweight='bold')\n",
    "            ax.set_title('Fitness Range Evolution', fontsize=12, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "            \n",
    "            # Standard deviation\n",
    "            ax = axes[1, 1]\n",
    "            ax.plot(generations, std_fitness, 'o-', linewidth=2, markersize=6, color='orange')\n",
    "            ax.set_xlabel('Generation', fontsize=11, fontweight='bold')\n",
    "            ax.set_ylabel('Standard Deviation', fontsize=11, fontweight='bold')\n",
    "            ax.set_title('Population Diversity (Std Dev of Fitness)', fontsize=12, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save plot\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            save_path = save_dir / f\"ga_evolution_{timestamp}.png\"\n",
    "            plt.savefig(str(save_path), dpi=100, bbox_inches='tight')\n",
    "            self.logger.info(f\" Evolution plot saved: {save_path.name}\")\n",
    "            \n",
    "            plt.show(block=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create evolution plot: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MATPLOTLIB WINDOW PLOT (keeping original visualization code)\n",
    "# =============================================================================\n",
    "\n",
    "class WindowPlotManager:\n",
    "    \"\"\"Manages creation of matplotlib window-based plots including CV results.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TabNetConfig, logger: Logger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.figures = []\n",
    "    \n",
    "    def create_training_history_plot(self, history: Dict) -> plt.Figure:\n",
    "        \"\"\"Create training history plot in a window.\"\"\"\n",
    "        if not history:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            fig = plt.figure(figsize=(16, 10))\n",
    "            fig.canvas.manager.set_window_title('TabNet Training History')\n",
    "            gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\n",
    "            \n",
    "            epochs = list(range(1, len(history.get('loss', [])) + 1))\n",
    "            \n",
    "            # Loss plot\n",
    "            ax1 = fig.add_subplot(gs[0, 0])\n",
    "            if 'loss' in history:\n",
    "                ax1.plot(epochs, history['loss'], 'o-', label='Training Loss', linewidth=2, markersize=4)\n",
    "            if 'val_loss' in history:\n",
    "                ax1.plot(epochs, history['val_loss'], 's-', label='Validation Loss', linewidth=2, markersize=4)\n",
    "            ax1.set_xlabel('Epoch', fontsize=11)\n",
    "            ax1.set_ylabel('Loss', fontsize=11)\n",
    "            ax1.set_title('Training & Validation Loss', fontsize=13, fontweight='bold')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Accuracy plot\n",
    "            ax2 = fig.add_subplot(gs[0, 1])\n",
    "            if 'accuracy' in history:\n",
    "                ax2.plot(epochs, history['accuracy'], 'o-', label='Training Accuracy', linewidth=2, markersize=4)\n",
    "            if 'val_accuracy' in history:\n",
    "                ax2.plot(epochs, history['val_accuracy'], 's-', label='Validation Accuracy', linewidth=2, markersize=4)\n",
    "            ax2.set_xlabel('Epoch', fontsize=11)\n",
    "            ax2.set_ylabel('Accuracy', fontsize=11)\n",
    "            ax2.set_title('Training & Validation Accuracy', fontsize=13, fontweight='bold')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # AUC plot\n",
    "            ax3 = fig.add_subplot(gs[1, 0])\n",
    "            if 'auc' in history:\n",
    "                ax3.plot(epochs, history['auc'], 'o-', label='Training AUC', linewidth=2, markersize=4)\n",
    "            if 'val_auc' in history:\n",
    "                ax3.plot(epochs, history['val_auc'], 's-', label='Validation AUC', linewidth=2, markersize=4)\n",
    "            ax3.set_xlabel('Epoch', fontsize=11)\n",
    "            ax3.set_ylabel('AUC', fontsize=11)\n",
    "            ax3.set_title('Training & Validation AUC', fontsize=13, fontweight='bold')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Learning rate plot\n",
    "            ax4 = fig.add_subplot(gs[1, 1])\n",
    "            if 'lr' in history:\n",
    "                ax4.plot(epochs, history['lr'], 'o-', label='Learning Rate', linewidth=2, markersize=4, color='purple')\n",
    "            ax4.set_xlabel('Epoch', fontsize=11)\n",
    "            ax4.set_ylabel('Learning Rate', fontsize=11)\n",
    "            ax4.set_title('Learning Rate Schedule', fontsize=13, fontweight='bold')\n",
    "            ax4.legend()\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            self.figures.append(fig)\n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create training history plot: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_metrics_bar_chart(self, metrics: Dict) -> plt.Figure:\n",
    "        \"\"\"Create bar chart for model metrics.\"\"\"\n",
    "        try:\n",
    "            categories = ['Accuracy', 'AUC', 'Sensitivity', 'Specificity', 'Precision', 'F1 Score']\n",
    "            values = [\n",
    "                metrics.get('accuracy', 0),\n",
    "                metrics.get('auc', 0),\n",
    "                metrics.get('sensitivity', 0),\n",
    "                metrics.get('specificity', 0),\n",
    "                metrics.get('precision', 0),\n",
    "                metrics.get('f1', 0)\n",
    "            ]\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            fig.canvas.manager.set_window_title('Performance Metrics')\n",
    "            \n",
    "            colors = plt.cm.RdYlGn(np.array(values))\n",
    "            bars = ax.bar(categories, values, color=colors, edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "            \n",
    "            ax.set_ylabel('Score', fontsize=13, fontweight='bold')\n",
    "            ax.set_xlabel('Metric', fontsize=13, fontweight='bold')\n",
    "            ax.set_title('Model Performance Metrics', fontsize=15, fontweight='bold', pad=20)\n",
    "            ax.set_ylim(0, 1.05)\n",
    "            ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "            ax.set_axisbelow(True)\n",
    "            \n",
    "            for bar, value in zip(bars, values):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{value:.4f}',\n",
    "                       ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "            \n",
    "            ax.axhline(y=0.8, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='0.8 threshold')\n",
    "            ax.axhline(y=0.9, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='0.9 threshold')\n",
    "            \n",
    "            plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\", fontsize=11)\n",
    "            ax.legend(loc='lower right', fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            self.figures.append(fig)\n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create metrics bar chart: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_and_display_plot(self, fig: plt.Figure, filepath: Path, plot_name: str) -> None:\n",
    "        \"\"\"Save plot and display in window.\"\"\"\n",
    "        if fig is None:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            if self.config.save_plots:\n",
    "                png_path = filepath.with_suffix('.png')\n",
    "                fig.savefig(str(png_path), dpi=self.config.plot_dpi, bbox_inches='tight')\n",
    "                self.logger.debug(f\"   Saved: {png_path.name}\")\n",
    "            \n",
    "            if self.config.display_plots:\n",
    "                self.logger.debug(f\"   Prepared: {plot_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to save/display plot {filepath.name}: {e}\")\n",
    "    \n",
    "    def show_all_plots(self):\n",
    "        \"\"\"Display all created plots in separate windows.\"\"\"\n",
    "        if self.config.display_plots and self.figures:\n",
    "            self.logger.info(f\"\\n Opening {len(self.figures)} plot windows...\")\n",
    "            self.logger.info(\"   Close plot windows to continue...\")\n",
    "            plt.show(block=self.config.block_on_plot)\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINER CLASS WITH GA OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "class TabNetTrainer:\n",
    "    \"\"\"Comprehensive TabNet trainer with GA optimization and cross-validation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_config: TabNetConfig, data_config: DataConfig):\n",
    "        self.config = model_config\n",
    "        self.data_config = data_config\n",
    "        self.logger = Logger(\"TabNetTrainer\")\n",
    "        self.device = None\n",
    "        self.model = None\n",
    "        self.feature_names = []\n",
    "        self.results = {}\n",
    "        self.cv_results = None\n",
    "        self.ga_results = None\n",
    "        \n",
    "        self.data_processor = DataProcessor(data_config, self.logger)\n",
    "        self.metrics_computer = MetricsComputer()\n",
    "        self.plot_manager = WindowPlotManager(model_config, self.logger)\n",
    "        \n",
    "        self._setup_environment()\n",
    "    \n",
    "    def _setup_environment(self):\n",
    "        \"\"\"Setup device and computational environment.\"\"\"\n",
    "        set_random_seeds(self.config.random_seed)\n",
    "        self.logger.info(f\" Random seeds set to {self.config.random_seed}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "            torch.backends.cudnn.deterministic = self.config.deterministic\n",
    "            torch.backends.cudnn.benchmark = not self.config.deterministic\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
    "            self.logger.info(f\" Using GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            self.logger.info(\" Using CPU\")\n",
    "            \n",
    "            self.config.batch_size = min(self.config.batch_size, 64)\n",
    "            self.config.virtual_batch_size = min(self.config.virtual_batch_size, 32)\n",
    "            self.logger.info(f\"   Adjusted batch sizes for CPU: batch={self.config.batch_size}, virtual={self.config.virtual_batch_size}\")\n",
    "    \n",
    "    def _create_model(self, hyperparams: Optional[Dict] = None) -> TabNetClassifier:\n",
    "        \"\"\"Create and configure TabNet model.\"\"\"\n",
    "        if not _HAS_TABNET:\n",
    "            raise RuntimeError(\"pytorch-tabnet is not available. Install with: pip install pytorch-tabnet\")\n",
    "        \n",
    "        # Use provided hyperparams or config defaults\n",
    "        if hyperparams is None:\n",
    "            hyperparams = {\n",
    "                'n_d': self.config.n_d,\n",
    "                'n_a': self.config.n_a,\n",
    "                'n_steps': self.config.n_steps,\n",
    "                'gamma': self.config.gamma,\n",
    "                'lambda_sparse': self.config.lambda_sparse,\n",
    "                'lr': self.config.lr,\n",
    "                'momentum': self.config.momentum,\n",
    "                'batch_size': self.config.batch_size,\n",
    "                'n_independent': self.config.n_independent,\n",
    "                'n_shared': self.config.n_shared\n",
    "            }\n",
    "        \n",
    "        model = TabNetClassifier(\n",
    "            n_d=hyperparams['n_d'],\n",
    "            n_a=hyperparams['n_a'],\n",
    "            n_steps=hyperparams['n_steps'],\n",
    "            gamma=hyperparams['gamma'],\n",
    "            lambda_sparse=hyperparams['lambda_sparse'],\n",
    "            momentum=hyperparams['momentum'],\n",
    "            n_independent=hyperparams['n_independent'],\n",
    "            n_shared=hyperparams['n_shared'],\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params={\"lr\": hyperparams['lr'], \"weight_decay\": 1e-5},\n",
    "            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "            scheduler_params={\"step_size\": 10, \"gamma\": 0.9},\n",
    "            device_name=str(self.device),\n",
    "            verbose=0,\n",
    "            seed=self.config.random_seed\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, X_train: np.ndarray, X_val: np.ndarray, \n",
    "             y_train: np.ndarray, y_val: np.ndarray,\n",
    "             hyperparams: Optional[Dict] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Train the TabNet model with optional optimized hyperparameters.\"\"\"\n",
    "        with self.logger.log_section(\"MODEL TRAINING\"):\n",
    "            \n",
    "            # Use optimized hyperparameters if available\n",
    "            if hyperparams is not None:\n",
    "                self.logger.info(\"Using optimized hyperparameters from genetic algorithm\")\n",
    "                for param, value in hyperparams.items():\n",
    "                    self.logger.info(f\"   {param:15s}: {value}\")\n",
    "                self.model = self._create_model(hyperparams)\n",
    "                batch_size = hyperparams['batch_size']\n",
    "                virtual_batch_size = min(batch_size // 8, 128)\n",
    "            else:\n",
    "                self.logger.info(\"Using default configuration hyperparameters\")\n",
    "                self.model = self._create_model()\n",
    "                batch_size = self.config.batch_size\n",
    "                virtual_batch_size = self.config.virtual_batch_size\n",
    "            \n",
    "            self.logger.info(f\"\\n Training Configuration:\")\n",
    "            self.logger.info(f\"   Max epochs: {self.config.max_epochs}\")\n",
    "            self.logger.info(f\"   Patience: {self.config.patience}\")\n",
    "            self.logger.info(f\"   Batch size: {batch_size}\")\n",
    "            self.logger.info(f\"   Virtual batch size: {virtual_batch_size}\")\n",
    "            \n",
    "            try:\n",
    "                with timer(self.logger, \"Model training\"):\n",
    "                    self.model.fit(\n",
    "                        X_train, y_train,\n",
    "                        eval_set=[(X_val, y_val)],\n",
    "                        eval_name=[\"val\"],\n",
    "                        eval_metric=[\"auc\", \"accuracy\"],\n",
    "                        max_epochs=self.config.max_epochs,\n",
    "                        patience=self.config.patience,\n",
    "                        batch_size=batch_size,\n",
    "                        virtual_batch_size=virtual_batch_size,\n",
    "                        drop_last=False,\n",
    "                        num_workers=0,\n",
    "                    )\n",
    "                \n",
    "                self.logger.info(f\"âœ“ Training completed at epoch {self.model.best_epoch}\")\n",
    "                return self.model.history or {}\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Training failed: {e}\")\n",
    "                raise\n",
    "    \n",
    "    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate the trained model.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model must be trained before evaluation\")\n",
    "        \n",
    "        with self.logger.log_section(\" MODEL EVALUATION ON TEST SET\"):\n",
    "            with timer(self.logger, \"Model evaluation\"):\n",
    "                y_pred = self.model.predict(X_test)\n",
    "                y_proba = self.model.predict_proba(X_test)[:, 1]\n",
    "                metrics = self.metrics_computer.compute_all_metrics(y_test, y_pred, y_proba)\n",
    "            \n",
    "            self.logger.info(self.metrics_computer.format_metrics_summary(metrics, \"TEST SET \"))\n",
    "            \n",
    "            self.logger.info(\"\\n CLASSIFICATION REPORT:\")\n",
    "            print(classification_report(y_test, y_pred, \n",
    "                                      target_names=['Benign', 'Malignant'], \n",
    "                                      digits=4))\n",
    "            \n",
    "            return metrics\n",
    "    \n",
    "    def create_visualizations(self, metrics: Dict, history: Dict, \n",
    "                            X_train: np.ndarray, y_train: np.ndarray, \n",
    "                            save_dir: Path) -> None:\n",
    "        \"\"\"Create all visualizations.\"\"\"\n",
    "        with self.logger.log_section(\"GENERATING VISUALIZATIONS\"):\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            \n",
    "            # Training History\n",
    "            history_fig = self.plot_manager.create_training_history_plot(history)\n",
    "            if history_fig:\n",
    "                self.plot_manager.save_and_display_plot(\n",
    "                    history_fig, \n",
    "                    save_dir / f\"training_history_{timestamp}\",\n",
    "                    \"Training History\"\n",
    "                )\n",
    "            \n",
    "            # Metrics Bar Chart\n",
    "            metrics_fig = self.plot_manager.create_metrics_bar_chart(metrics)\n",
    "            if metrics_fig:\n",
    "                self.plot_manager.save_and_display_plot(\n",
    "                    metrics_fig, \n",
    "                    save_dir / f\"metrics_barchart_{timestamp}\",\n",
    "                    \"Metrics Bar Chart\"\n",
    "                )\n",
    "            \n",
    "            # Show all plots\n",
    "            self.plot_manager.show_all_plots()\n",
    "            \n",
    "            self.logger.info(\"âœ“ All visualizations generated and displayed\")\n",
    "    \n",
    "    def save_experiment(self, metrics: Dict, history: Dict, save_dir: Path) -> None:\n",
    "        \"\"\"Save complete experiment results including GA results.\"\"\"\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        experiment_data = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'config': {\n",
    "                'model': asdict(self.config),\n",
    "                'data': asdict(self.data_config)\n",
    "            },\n",
    "            'metrics': {k: v for k, v in metrics.items() \n",
    "                       if k not in ['confusion_matrix', 'roc_curve']},\n",
    "            'confusion_matrix': metrics.get('confusion_matrix', np.array([])).tolist(),\n",
    "            'feature_names': self.feature_names,\n",
    "            'training_info': {\n",
    "                'best_epoch': getattr(self.model, 'best_epoch', -1) if self.model else -1,\n",
    "                'total_params': sum(p.numel() for p in self.model.network.parameters()) if self.model else 0,\n",
    "                'trainable_params': sum(p.numel() for p in self.model.network.parameters() \n",
    "                                      if p.requires_grad) if self.model else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add GA results if available\n",
    "        if self.ga_results is not None:\n",
    "            experiment_data['genetic_algorithm'] = {\n",
    "                'best_hyperparams': self.ga_results['best_hyperparams'],\n",
    "                'best_fitness': self.ga_results['best_fitness'],\n",
    "                'hall_of_fame': self.ga_results['hall_of_fame'],\n",
    "                'final_generation_stats': self.ga_results['generation_stats'][-1] if self.ga_results['generation_stats'] else {}\n",
    "            }\n",
    "        \n",
    "        results_path = save_dir / f\"experiment_results_{timestamp}.json\"\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(experiment_data, f, indent=2)\n",
    "        \n",
    "        # Save GA results separately if available\n",
    "        if self.ga_results is not None:\n",
    "            ga_path = save_dir / f\"ga_optimization_{timestamp}.pkl\"\n",
    "            with open(ga_path, 'wb') as f:\n",
    "                pickle.dump(self.ga_results, f)\n",
    "            self.logger.info(f\"GA results saved: {ga_path.name}\")\n",
    "        \n",
    "        if self.model:\n",
    "            model_path = save_dir / f\"tabnet_model_{timestamp}.pth\"\n",
    "            torch.save(self.model.network.state_dict(), model_path)\n",
    "            self.logger.info(f\"Model saved: {model_path.name}\")\n",
    "        \n",
    "        self.logger.info(f\"Results saved: {results_path.name}\")\n",
    "    \n",
    "    def run_experiment(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run complete training experiment with GA optimization.\"\"\"\n",
    "        with self.logger.log_section(\"TABNET TRAINING EXPERIMENT WITH GA OPTIMIZATION\"):\n",
    "            try:\n",
    "                # 1. Load and preprocess data\n",
    "                with self.logger.log_section(\"DATA LOADING & PREPROCESSING\"):\n",
    "                    X_train, X_val, X_test, y_train, y_val, y_test = \\\n",
    "                        self.data_processor.load_and_preprocess(\n",
    "                            Path(self.config.dataset_path),\n",
    "                            self.config.random_seed\n",
    "                        )\n",
    "                    self.feature_names = self.data_processor.feature_names\n",
    "                \n",
    "                # 2. Run genetic algorithm optimization (if enabled)\n",
    "                optimized_hyperparams = None\n",
    "                if self.config.use_ga_optimization:\n",
    "                    ga_optimizer = GeneticOptimizer(\n",
    "                        self.config, self.data_config,\n",
    "                        X_train, X_val, y_train, y_val,\n",
    "                        self.logger\n",
    "                    )\n",
    "                    \n",
    "                    self.ga_results = ga_optimizer.optimize()\n",
    "                    optimized_hyperparams = self.ga_results['best_hyperparams']\n",
    "                    \n",
    "                    # Plot evolution progress\n",
    "                    results_dir = Path(self.config.results_dir)\n",
    "                    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    ga_optimizer.plot_evolution_progress(results_dir)\n",
    "                \n",
    "                # 3. Train final model with optimized hyperparameters\n",
    "                history = self.train(X_train, X_val, y_train, y_val, optimized_hyperparams)\n",
    "                \n",
    "                # 4. Evaluate model on test set\n",
    "                metrics = self.evaluate(X_test, y_test)\n",
    "                \n",
    "                # 5. Create visualizations\n",
    "                results_dir = Path(self.config.results_dir)\n",
    "                self.create_visualizations(metrics, history, X_train, y_train, results_dir)\n",
    "                \n",
    "                # 6. Save experiment\n",
    "                self.save_experiment(metrics, history, results_dir)\n",
    "                \n",
    "                self.results = {\n",
    "                    'metrics': metrics,\n",
    "                    'history': history,\n",
    "                    'feature_names': self.feature_names,\n",
    "                    'ga_results': self.ga_results,\n",
    "                    'optimized_hyperparams': optimized_hyperparams\n",
    "                }\n",
    "                \n",
    "                with self.logger.log_section(\"EXPERIMENT COMPLETED SUCCESSFULLY\"):\n",
    "                    if self.ga_results:\n",
    "                        self.logger.info(\"Genetic Algorithm Results:\")\n",
    "                        self.logger.info(f\"   Best Fitness: {self.ga_results['best_fitness']:.6f}\")\n",
    "                        self.logger.info(f\"   Generations: {len(self.ga_results['generation_stats'])}\")\n",
    "                    \n",
    "                    self.logger.info(f\"\\n Final Test Performance:\")\n",
    "                    self.logger.info(f\"   Accuracy:    {metrics['accuracy']:.4f}\")\n",
    "                    self.logger.info(f\"   AUC-ROC:     {metrics['auc']:.4f}\")\n",
    "                    self.logger.info(f\"   Sensitivity: {metrics['sensitivity']:.4f}\")\n",
    "                    self.logger.info(f\"   Specificity: {metrics['specificity']:.4f}\")\n",
    "                    self.logger.info(f\"   F1 Score:    {metrics['f1']:.4f}\")\n",
    "                    self.logger.info(f\"\\n Results saved in: {results_dir}/\")\n",
    "                \n",
    "                return self.results\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\" Experiment failed: {e}\")\n",
    "                raise\n",
    "            finally:\n",
    "                clear_memory()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def run_tabnet_training(dataset_path: str = \"C:/Users/awwal/Desktop/MLEA_experiments/data.csv\",\n",
    "                       results_dir: str = \"tabnet_results_ga\",\n",
    "                       use_ga_optimization: bool = True,\n",
    "                       ga_population_size: int = 20,\n",
    "                       ga_generations: int = 15,\n",
    "                       display_plots: bool = True,\n",
    "                       block_on_plot: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run TabNet training experiment with genetic algorithm optimization.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to CSV dataset\n",
    "        results_dir: Directory to save results\n",
    "        use_ga_optimization: Whether to use genetic algorithm for hyperparameter tuning\n",
    "        ga_population_size: Population size for GA\n",
    "        ga_generations: Number of generations for GA\n",
    "        display_plots: Whether to display plots in windows\n",
    "        block_on_plot: Whether to pause execution until windows are closed\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing metrics, history, feature names, and GA results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   TABNET TRAINING WITH GENETIC ALGORITHM OPTIMIZATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if not _HAS_TABNET:\n",
    "        print(\"\\n pytorch-tabnet is required but not installed.\")\n",
    "        print(\"   Install with: pip install pytorch-tabnet\")\n",
    "        raise ImportError(\"pytorch-tabnet not available\")\n",
    "    \n",
    "    # Configuration\n",
    "    model_config = TabNetConfig(\n",
    "        dataset_path=dataset_path,\n",
    "        results_dir=results_dir,\n",
    "        max_epochs=50,\n",
    "        patience=10,\n",
    "        use_ga_optimization=use_ga_optimization,\n",
    "        ga_population_size=ga_population_size,\n",
    "        ga_generations=ga_generations,\n",
    "        ga_mutation_rate=0.3,\n",
    "        ga_crossover_rate=0.7,\n",
    "        ga_tournament_size=3,\n",
    "        display_plots=display_plots,\n",
    "        save_plots=True,\n",
    "        plot_dpi=100,\n",
    "        block_on_plot=block_on_plot,\n",
    "        use_cross_validation=False  # GA already provides validation\n",
    "    )\n",
    "    \n",
    "    data_config = DataConfig(\n",
    "        target_column=\"diagnosis\",\n",
    "        test_size=0.2,\n",
    "        validation_size=0.15,  # Larger validation set for GA\n",
    "        scale_features=True,\n",
    "        handle_missing=\"mean\"\n",
    "    )\n",
    "    \n",
    "    # Create and run trainer\n",
    "    trainer = TabNetTrainer(model_config, data_config)\n",
    "    results = trainer.run_experiment()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for command-line execution.\"\"\"\n",
    "    try:\n",
    "        results = run_tabnet_training()\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Experiment failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        get_ipython()\n",
    "        print(\"\\n Running in Jupyter/IPython environment\")\n",
    "        results = run_tabnet_training(block_on_plot=False)\n",
    "        print(\"\\n Experiment completed! Results stored in 'results' variable\")\n",
    "        \n",
    "        # Display summary\n",
    "        if results.get('ga_results'):\n",
    "            print(\"\\n GA Optimization Summary:\")\n",
    "            print(f\"   Best Fitness: {results['ga_results']['best_fitness']:.6f}\")\n",
    "            print(f\"\\n Optimized Hyperparameters:\")\n",
    "            for param, value in results['optimized_hyperparams'].items():\n",
    "                print(f\"   {param:15s}: {value}\")\n",
    "    except NameError:\n",
    "        sys.exit(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
