{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c0617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49adc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¬ Running in Jupyter/IPython environment\n",
      "\n",
      "======================================================================\n",
      "  TABNET TRAINING WITH CROSS-VALIDATION & OVERFITTING DETECTION\n",
      "======================================================================\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer:  Random seeds set to 42\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: Using CPU\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer:    Adjusted batch sizes for CPU: batch=64, virtual=32\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer:  TABNET TRAINING EXPERIMENT\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer:  DATA LOADING & PREPROCESSING\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: Starting: Loading dataset from C:\\Users\\awwal\\Desktop\\MLEA_experiments\\data.csv\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer:    Shape: (569, 33)\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: âœ“ Completed: Loading dataset from C:\\Users\\awwal\\Desktop\\MLEA_experiments\\data.csv (0.02s)\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer:    Class distribution: Benign=357 (62.7%), Malignant=212 (37.3%)\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer:    Features: 30\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer:    Train set: 398 samples\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer:    Validation set: 57 samples\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer:    Test set: 114 samples\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: Starting: Feature scaling\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: âœ“ Completed: Feature scaling (0.01s)\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: \n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer:  MODEL TRAINING\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: Using Cross-Validation mode\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer:  5-FOLD CROSS-VALIDATION\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer:   FOLD 1/5\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer:    Train: 364 samples\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer:    Val:   91 samples\n",
      "2025-11-12 17:38:04 [INFO] TabNetTrainer: Starting: Fold 1 training\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_accuracy = 0.96703\n",
      "2025-11-12 17:38:20 [INFO] TabNetTrainer: âœ“ Completed: Fold 1 training (15.38s)\n",
      "2025-11-12 17:38:20 [INFO] TabNetTrainer:    Fold 1 completed at epoch 11\n",
      "2025-11-12 17:38:20 [INFO] TabNetTrainer: \n",
      "   Fold 1 Results:\n",
      "2025-11-12 17:38:20 [INFO] TabNetTrainer:    Accuracy: 0.9670\n",
      "2025-11-12 17:38:20 [INFO] TabNetTrainer:    AUC:      0.9933\n",
      "2025-11-12 17:38:20 [INFO] TabNetTrainer:    F1:       0.9552\n",
      "2025-11-12 17:38:20 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-12 17:38:20 [INFO] TabNetTrainer:   FOLD 2/5\n",
      "2025-11-12 17:38:20 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-12 17:38:20 [INFO] TabNetTrainer:    Train: 364 samples\n",
      "2025-11-12 17:38:20 [INFO] TabNetTrainer:    Val:   91 samples\n",
      "2025-11-12 17:38:20 [INFO] TabNetTrainer: Starting: Fold 2 training\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_accuracy = 0.92308\n",
      "2025-11-12 17:38:39 [INFO] TabNetTrainer: âœ“ Completed: Fold 2 training (18.06s)\n",
      "2025-11-12 17:38:39 [INFO] TabNetTrainer:    Fold 2 completed at epoch 15\n",
      "2025-11-12 17:38:39 [INFO] TabNetTrainer: \n",
      "   Fold 2 Results:\n",
      "2025-11-12 17:38:39 [INFO] TabNetTrainer:    Accuracy: 0.9231\n",
      "2025-11-12 17:38:39 [INFO] TabNetTrainer:    AUC:      0.9902\n",
      "2025-11-12 17:38:39 [INFO] TabNetTrainer:    F1:       0.8923\n",
      "2025-11-12 17:38:39 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-12 17:38:39 [INFO] TabNetTrainer:   FOLD 3/5\n",
      "2025-11-12 17:38:39 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-12 17:38:39 [INFO] TabNetTrainer:    Train: 364 samples\n",
      "2025-11-12 17:38:39 [INFO] TabNetTrainer:    Val:   91 samples\n",
      "2025-11-12 17:38:39 [INFO] TabNetTrainer: Starting: Fold 3 training\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_accuracy = 1.0\n",
      "2025-11-12 17:38:52 [INFO] TabNetTrainer: âœ“ Completed: Fold 3 training (12.98s)\n",
      "2025-11-12 17:38:52 [INFO] TabNetTrainer:    Fold 3 completed at epoch 8\n",
      "2025-11-12 17:38:52 [INFO] TabNetTrainer: \n",
      "   Fold 3 Results:\n",
      "2025-11-12 17:38:52 [INFO] TabNetTrainer:    Accuracy: 1.0000\n",
      "2025-11-12 17:38:52 [INFO] TabNetTrainer:    AUC:      1.0000\n",
      "2025-11-12 17:38:52 [INFO] TabNetTrainer:    F1:       1.0000\n",
      "2025-11-12 17:38:53 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-12 17:38:53 [INFO] TabNetTrainer:   FOLD 4/5\n",
      "2025-11-12 17:38:53 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-12 17:38:53 [INFO] TabNetTrainer:    Train: 364 samples\n",
      "2025-11-12 17:38:53 [INFO] TabNetTrainer:    Val:   91 samples\n",
      "2025-11-12 17:38:53 [INFO] TabNetTrainer: Starting: Fold 4 training\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_accuracy = 0.96703\n",
      "2025-11-12 17:39:11 [INFO] TabNetTrainer: âœ“ Completed: Fold 4 training (18.37s)\n",
      "2025-11-12 17:39:11 [INFO] TabNetTrainer:    Fold 4 completed at epoch 15\n",
      "2025-11-12 17:39:11 [INFO] TabNetTrainer: \n",
      "   Fold 4 Results:\n",
      "2025-11-12 17:39:11 [INFO] TabNetTrainer:    Accuracy: 0.9670\n",
      "2025-11-12 17:39:11 [INFO] TabNetTrainer:    AUC:      0.9948\n",
      "2025-11-12 17:39:11 [INFO] TabNetTrainer:    F1:       0.9552\n",
      "2025-11-12 17:39:11 [INFO] TabNetTrainer: \n",
      "============================================================\n",
      "2025-11-12 17:39:11 [INFO] TabNetTrainer:   FOLD 5/5\n",
      "2025-11-12 17:39:11 [INFO] TabNetTrainer: ============================================================\n",
      "2025-11-12 17:39:11 [INFO] TabNetTrainer:    Train: 364 samples\n",
      "2025-11-12 17:39:11 [INFO] TabNetTrainer:    Val:   91 samples\n",
      "2025-11-12 17:39:11 [INFO] TabNetTrainer: Starting: Fold 5 training\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_accuracy = 0.94505\n",
      "2025-11-12 17:39:30 [INFO] TabNetTrainer: âœ“ Completed: Fold 5 training (18.84s)\n",
      "2025-11-12 17:39:30 [INFO] TabNetTrainer:    Fold 5 completed at epoch 16\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer: \n",
      "   Fold 5 Results:\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer:    Accuracy: 0.9451\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer:    AUC:      0.9422\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer:    F1:       0.9231\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer: \n",
      "======================================================================\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer:   CROSS-VALIDATION SUMMARY\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer:    Accuracy    : 0.9604 Â± 0.0256\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer:    Auc         : 0.9841 Â± 0.0212\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer:    Sensitivity : 0.9235 Â± 0.0513\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer:    Specificity : 0.9825 Â± 0.0111\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer:    Precision   : 0.9685 Â± 0.0204\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer:    F1          : 0.9452 Â± 0.0360\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer:    Npv         : 0.9562 Â± 0.0289\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer: \n",
      "   Best Fold: 3 (AUC: 1.0000)\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer: \n",
      " OVERFITTING ANALYSIS:\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer: No major overfitting indicators detected\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer: \n",
      "   Consistency Ratings:\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer:        Accuracy: Good\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer:        Auc: Good\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer:        F1: Good\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer: \n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer: \n",
      "======================================================================\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer:   TRAINING FINAL MODEL ON FULL TRAINING SET\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-12 17:39:31 [INFO] TabNetTrainer: Starting: Single training\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_accuracy = 0.98246\n",
      "2025-11-12 17:39:46 [INFO] TabNetTrainer: âœ“ Completed: Single training (15.27s)\n",
      "2025-11-12 17:39:46 [INFO] TabNetTrainer:    Single completed at epoch 7\n",
      "2025-11-12 17:39:46 [INFO] TabNetTrainer: \n",
      "2025-11-12 17:39:46 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-12 17:39:46 [INFO] TabNetTrainer:  MODEL EVALUATION ON TEST SET\n",
      "2025-11-12 17:39:46 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-12 17:39:46 [INFO] TabNetTrainer: Starting: Model evaluation\n",
      "2025-11-12 17:39:46 [INFO] TabNetTrainer: âœ“ Completed: Model evaluation (0.16s)\n",
      "2025-11-12 17:39:46 [INFO] TabNetTrainer: \n",
      " TEST SET MODEL PERFORMANCE METRICS:\n",
      "   Accuracy:    0.9561\n",
      "   AUC-ROC:     0.9937\n",
      "   Sensitivity: 0.9286\n",
      "   Specificity: 0.9722\n",
      "   Precision:   0.9512\n",
      "   NPV:         0.9589\n",
      "   F1 Score:    0.9398\n",
      "\n",
      "ðŸ“‹ CONFUSION MATRIX:\n",
      "   TN:   70  FP:    2\n",
      "   FN:    3  TP:   39\n",
      "2025-11-12 17:39:46 [INFO] TabNetTrainer: \n",
      " CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign     0.9589    0.9722    0.9655        72\n",
      "   Malignant     0.9512    0.9286    0.9398        42\n",
      "\n",
      "    accuracy                         0.9561       114\n",
      "   macro avg     0.9551    0.9504    0.9526       114\n",
      "weighted avg     0.9561    0.9561    0.9560       114\n",
      "\n",
      "2025-11-12 17:39:46 [INFO] TabNetTrainer: \n",
      "2025-11-12 17:39:46 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-12 17:39:46 [INFO] TabNetTrainer:  GENERATING VISUALIZATIONS\n",
      "2025-11-12 17:39:46 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-12 17:39:46 [INFO] TabNetTrainer: Creating Cross-Validation plots...\n",
      "2025-11-12 17:39:51 [INFO] TabNetTrainer: Creating standard training plots...\n",
      "2025-11-12 17:39:52 [ERROR] TabNetTrainer: Failed to create training history plot: 'History' object has no attribute 'get'\n",
      "2025-11-12 17:39:55 [INFO] TabNetTrainer: \n",
      "ðŸ“Š Opening 7 plot windows...\n",
      "2025-11-12 17:39:55 [INFO] TabNetTrainer:    Close plot windows to continue...\n",
      "2025-11-12 17:39:55 [INFO] TabNetTrainer: All visualizations generated and displayed\n",
      "2025-11-12 17:39:55 [INFO] TabNetTrainer: \n",
      "2025-11-12 17:39:55 [INFO] TabNetTrainer:  Model saved: tabnet_model_20251112_173955.pth\n",
      "2025-11-12 17:39:55 [INFO] TabNetTrainer: Results saved: experiment_results_20251112_173955.json\n",
      "2025-11-12 17:39:55 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-12 17:39:55 [INFO] TabNetTrainer:  EXPERIMENT COMPLETED SUCCESSFULLY\n",
      "2025-11-12 17:39:55 [INFO] TabNetTrainer: ======================================================================\n",
      "2025-11-12 17:39:55 [INFO] TabNetTrainer:  Cross-Validation Results:\n",
      "2025-11-12 17:39:55 [INFO] TabNetTrainer:    Mean AUC:      0.9841 Â± 0.0212\n",
      "2025-11-12 17:39:55 [INFO] TabNetTrainer:    Mean Accuracy: 0.9604 Â± 0.0256\n",
      "2025-11-12 17:39:55 [INFO] TabNetTrainer:  Final Test Accuracy: 0.9561\n",
      "2025-11-12 17:39:55 [INFO] TabNetTrainer:  Final Test AUC-ROC:  0.9937\n",
      "2025-11-12 17:39:55 [INFO] TabNetTrainer:  Results saved in: tabnet_results_cv/\n",
      "2025-11-12 17:39:55 [INFO] TabNetTrainer: \n",
      "2025-11-12 17:39:56 [INFO] TabNetTrainer: \n",
      "\n",
      " Experiment completed! Results stored in 'results' variable\n",
      "\n",
      "Cross-Validation Summary:\n",
      "   Accuracy: 0.9604 Â± 0.0256\n",
      "   Auc: 0.9841 Â± 0.0212\n",
      "   F1: 0.9452 Â± 0.0360\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gc\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, roc_auc_score, \n",
    "    classification_report, roc_curve\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Matplotlib imports for window plotting\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # Use TkAgg backend for Windows GUI windows\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Conditional import for TabNet\n",
    "try:\n",
    "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "    _HAS_TABNET = True\n",
    "except ImportError:\n",
    "    TabNetClassifier = None\n",
    "    _HAS_TABNET = False\n",
    "\n",
    "# =============================================================================\n",
    "# ENUMS AND CONSTANTS\n",
    "# =============================================================================\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Default column names to drop\n",
    "DEFAULT_DROP_COLUMNS = [\"id\", \"ID\", \"Id\", \"patient_id\", \"Unnamed: 0\"]\n",
    "\n",
    "# Target column candidates\n",
    "TARGET_CANDIDATES = [\"diagnosis\", \"Diagnosis\", \"target\", \"Target\", \"class\", \"Class\"]\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION CLASSES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TabNetConfig:\n",
    "    \"\"\"Configuration for TabNet model and training with validation.\"\"\"\n",
    "    \n",
    "    # Model architecture\n",
    "    n_d: int = 16\n",
    "    n_a: int = 16\n",
    "    n_steps: int = 5\n",
    "    gamma: float = 1.5\n",
    "    lambda_sparse: float = 0.001\n",
    "    lr: float = 0.02\n",
    "    momentum: float = 0.02\n",
    "    n_independent: int = 2\n",
    "    n_shared: int = 2\n",
    "    \n",
    "    # Training parameters\n",
    "    max_epochs: int = 50\n",
    "    patience: int = 10\n",
    "    batch_size: int = 512\n",
    "    virtual_batch_size: int = 64\n",
    "    \n",
    "    # Cross-validation parameters\n",
    "    use_cross_validation: bool = True\n",
    "    n_folds: int = 5\n",
    "    cv_random_state: int = 42\n",
    "    \n",
    "    # Visualization\n",
    "    display_plots: bool = True  # Display plots in windows\n",
    "    save_plots: bool = True     # Save plots as PNG\n",
    "    plot_dpi: int = 100         # DPI for saved plots\n",
    "    block_on_plot: bool = False # Whether to block execution when showing plots\n",
    "    \n",
    "    # Environment\n",
    "    random_seed: int = 42\n",
    "    deterministic: bool = True\n",
    "    \n",
    "    # Paths\n",
    "    dataset_path: str = \"data.csv\"\n",
    "    results_dir: str = \"tabnet_results\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration parameters.\"\"\"\n",
    "        self._validate()\n",
    "    \n",
    "    def _validate(self):\n",
    "        \"\"\"Validate all configuration parameters.\"\"\"\n",
    "        if self.n_d <= 0 or self.n_a <= 0:\n",
    "            raise ValueError(f\"n_d and n_a must be positive (got n_d={self.n_d}, n_a={self.n_a})\")\n",
    "        \n",
    "        if self.n_steps <= 0:\n",
    "            raise ValueError(f\"n_steps must be positive (got {self.n_steps})\")\n",
    "        \n",
    "        if not 0 <= self.gamma <= 10:\n",
    "            raise ValueError(f\"gamma should be in [0, 10] (got {self.gamma})\")\n",
    "        \n",
    "        if not 0 <= self.lambda_sparse <= 1:\n",
    "            raise ValueError(f\"lambda_sparse should be in [0, 1] (got {self.lambda_sparse})\")\n",
    "        \n",
    "        if self.lr <= 0:\n",
    "            raise ValueError(f\"lr must be positive (got {self.lr})\")\n",
    "        \n",
    "        if self.max_epochs <= 0:\n",
    "            raise ValueError(f\"max_epochs must be positive (got {self.max_epochs})\")\n",
    "        \n",
    "        if self.patience <= 0:\n",
    "            raise ValueError(f\"patience must be positive (got {self.patience})\")\n",
    "        \n",
    "        if self.batch_size <= 0 or self.virtual_batch_size <= 0:\n",
    "            raise ValueError(\"batch_size and virtual_batch_size must be positive\")\n",
    "        \n",
    "        if self.virtual_batch_size > self.batch_size:\n",
    "            raise ValueError(\"virtual_batch_size must be <= batch_size\")\n",
    "        \n",
    "        if self.use_cross_validation and self.n_folds < 2:\n",
    "            raise ValueError(f\"n_folds must be >= 2 (got {self.n_folds})\")\n",
    "        \n",
    "        if not Path(self.dataset_path).exists():\n",
    "            raise FileNotFoundError(f\"Dataset not found: {self.dataset_path}\")\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Configuration for data loading and preprocessing with validation.\"\"\"\n",
    "    \n",
    "    target_column: Optional[str] = None\n",
    "    drop_columns: List[str] = field(default_factory=lambda: DEFAULT_DROP_COLUMNS.copy())\n",
    "    test_size: float = 0.2\n",
    "    validation_size: float = 0.1\n",
    "    scale_features: bool = True\n",
    "    handle_missing: str = \"mean\"  # \"mean\", \"median\", \"drop\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration parameters.\"\"\"\n",
    "        self._validate()\n",
    "    \n",
    "    def _validate(self):\n",
    "        \"\"\"Validate all configuration parameters.\"\"\"\n",
    "        if not 0 < self.test_size < 1:\n",
    "            raise ValueError(f\"test_size must be in (0, 1) (got {self.test_size})\")\n",
    "        \n",
    "        if not 0 < self.validation_size < 1:\n",
    "            raise ValueError(f\"validation_size must be in (0, 1) (got {self.validation_size})\")\n",
    "        \n",
    "        if self.test_size + self.validation_size >= 1:\n",
    "            raise ValueError(\"test_size + validation_size must be < 1\")\n",
    "        \n",
    "        if self.handle_missing not in [\"mean\", \"median\", \"drop\"]:\n",
    "            raise ValueError(f\"handle_missing must be 'mean', 'median', or 'drop' (got {self.handle_missing})\")\n",
    "\n",
    "# =============================================================================\n",
    "# CROSS-VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class CVResults:\n",
    "    \"\"\"Stores cross-validation results.\"\"\"\n",
    "    fold_metrics: List[Dict[str, float]] = field(default_factory=list)\n",
    "    fold_histories: List[Dict] = field(default_factory=list)\n",
    "    best_fold: int = -1\n",
    "    mean_metrics: Dict[str, float] = field(default_factory=dict)\n",
    "    std_metrics: Dict[str, float] = field(default_factory=dict)\n",
    "    \n",
    "    def add_fold_result(self, fold_idx: int, metrics: Dict, history: Dict):\n",
    "        \"\"\"Add results from a single fold.\"\"\"\n",
    "        self.fold_metrics.append(metrics)\n",
    "        self.fold_histories.append(history)\n",
    "    \n",
    "    def compute_statistics(self):\n",
    "        \"\"\"Compute mean and std across folds.\"\"\"\n",
    "        if not self.fold_metrics:\n",
    "            return\n",
    "        \n",
    "        # Extract numeric metrics\n",
    "        metric_keys = ['accuracy', 'auc', 'sensitivity', 'specificity', 'precision', 'f1', 'npv']\n",
    "        \n",
    "        for key in metric_keys:\n",
    "            values = [fold[key] for fold in self.fold_metrics if key in fold]\n",
    "            if values:\n",
    "                self.mean_metrics[key] = np.mean(values)\n",
    "                self.std_metrics[key] = np.std(values)\n",
    "        \n",
    "        # Find best fold based on AUC\n",
    "        if 'auc' in self.mean_metrics:\n",
    "            auc_scores = [fold.get('auc', 0) for fold in self.fold_metrics]\n",
    "            self.best_fold = int(np.argmax(auc_scores))\n",
    "    \n",
    "    def get_overfitting_analysis(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze overfitting by comparing training and validation performance.\"\"\"\n",
    "        analysis = {\n",
    "            'variance': {},\n",
    "            'consistency': {},\n",
    "            'overfitting_indicators': []\n",
    "        }\n",
    "        \n",
    "        # Compute coefficient of variation (CV) for each metric\n",
    "        for metric, mean_val in self.mean_metrics.items():\n",
    "            if metric in self.std_metrics and mean_val > 0:\n",
    "                cv = self.std_metrics[metric] / mean_val\n",
    "                analysis['variance'][metric] = cv\n",
    "                \n",
    "                # Flag high variance (CV > 0.1 means >10% relative variation)\n",
    "                if cv > 0.1:\n",
    "                    analysis['overfitting_indicators'].append(\n",
    "                        f\"High variance in {metric}: CV={cv:.3f}\"\n",
    "                    )\n",
    "        \n",
    "        # Check consistency across folds\n",
    "        for metric in ['accuracy', 'auc', 'f1']:\n",
    "            if metric in self.std_metrics:\n",
    "                std = self.std_metrics[metric]\n",
    "                analysis['consistency'][metric] = 'Good' if std < 0.05 else 'Moderate' if std < 0.10 else 'Poor'\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "class Logger:\n",
    "    \"\"\"Enhanced logger with context management.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, level: int = logging.INFO):\n",
    "        self.logger = logging.getLogger(name)\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler(sys.stdout)\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    "                datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "            self.logger.setLevel(level)\n",
    "    \n",
    "    @contextmanager\n",
    "    def log_section(self, title: str):\n",
    "        \"\"\"Context manager for logging sections.\"\"\"\n",
    "        self.logger.info(\"=\" * 70)\n",
    "        self.logger.info(f\" {title}\")\n",
    "        self.logger.info(\"=\" * 70)\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            self.logger.info(\"\")\n",
    "    \n",
    "    def info(self, msg: str):\n",
    "        self.logger.info(msg)\n",
    "    \n",
    "    def warning(self, msg: str):\n",
    "        self.logger.warning(msg)\n",
    "    \n",
    "    def error(self, msg: str):\n",
    "        self.logger.error(msg)\n",
    "    \n",
    "    def debug(self, msg: str):\n",
    "        self.logger.debug(msg)\n",
    "\n",
    "@contextmanager\n",
    "def timer(logger: Logger, operation: str):\n",
    "    \"\"\"Context manager for timing operations.\"\"\"\n",
    "    start_time = datetime.now()\n",
    "    logger.info(f\"Starting: {operation}\")\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        logger.info(f\"âœ“ Completed: {operation} ({elapsed:.2f}s)\")\n",
    "\n",
    "def set_random_seeds(seed: int):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    import random\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Aggressively clear memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            torch.cuda.synchronize()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PROCESSING CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Handles all data loading and preprocessing operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataConfig, logger: Logger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.scaler = None\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def load_dataset(self, filepath: Path) -> pd.DataFrame:\n",
    "        \"\"\"Load dataset from file with error handling.\"\"\"\n",
    "        if not filepath.exists():\n",
    "            raise FileNotFoundError(f\"Dataset not found: {filepath}\")\n",
    "        \n",
    "        try:\n",
    "            with timer(self.logger, f\"Loading dataset from {filepath}\"):\n",
    "                df = pd.read_csv(filepath)\n",
    "                self.logger.info(f\"   Shape: {df.shape}\")\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load dataset: {e}\")\n",
    "    \n",
    "    def identify_target_column(self, df: pd.DataFrame) -> str:\n",
    "        \"\"\"Identify the target column in the dataset.\"\"\"\n",
    "        if self.config.target_column is not None:\n",
    "            if self.config.target_column in df.columns:\n",
    "                return self.config.target_column\n",
    "            else:\n",
    "                raise ValueError(f\"Specified target column '{self.config.target_column}' not found\")\n",
    "        \n",
    "        for candidate in TARGET_CANDIDATES:\n",
    "            if candidate in df.columns:\n",
    "                self.logger.info(f\"Auto-detected target column: '{candidate}'\")\n",
    "                return candidate\n",
    "        \n",
    "        target_col = df.columns[-1]\n",
    "        self.logger.warning(f\"Using last column as target: '{target_col}'\")\n",
    "        return target_col\n",
    "    \n",
    "    def preprocess_target(self, target_series: pd.Series) -> np.ndarray:\n",
    "        \"\"\"Convert target to binary format with robust handling.\"\"\"\n",
    "        target_series = target_series.copy()\n",
    "        \n",
    "        if target_series.dtype == 'object' or target_series.dtype.name == 'category':\n",
    "            return self._preprocess_categorical_target(target_series)\n",
    "        \n",
    "        return self._preprocess_numeric_target(target_series)\n",
    "    \n",
    "    def _preprocess_categorical_target(self, target_series: pd.Series) -> np.ndarray:\n",
    "        \"\"\"Preprocess categorical target values.\"\"\"\n",
    "        normalized = target_series.astype(str).str.strip().str.lower()\n",
    "        \n",
    "        mapping = {\n",
    "            'm': 1, 'malignant': 1, '1': 1, '4': 1, 'positive': 1, 'yes': 1,\n",
    "            'b': 0, 'benign': 0, '0': 0, '2': 0, 'negative': 0, 'no': 0\n",
    "        }\n",
    "        \n",
    "        mapped = normalized.map(mapping)\n",
    "        \n",
    "        if not mapped.isna().any():\n",
    "            return mapped.astype(int).values\n",
    "        \n",
    "        numeric_vals = pd.to_numeric(normalized, errors='coerce')\n",
    "        if numeric_vals.notna().all():\n",
    "            return self._preprocess_numeric_target(numeric_vals)\n",
    "        \n",
    "        self.logger.warning(\"Using LabelEncoder for target conversion\")\n",
    "        encoder = LabelEncoder()\n",
    "        encoded = encoder.fit_transform(normalized)\n",
    "        \n",
    "        if len(encoder.classes_) != 2:\n",
    "            raise ValueError(f\"Expected binary target, got {len(encoder.classes_)} classes\")\n",
    "        \n",
    "        return encoded.astype(int)\n",
    "    \n",
    "    def _preprocess_numeric_target(self, target_series: pd.Series) -> np.ndarray:\n",
    "        \"\"\"Preprocess numeric target values.\"\"\"\n",
    "        numeric_vals = pd.to_numeric(target_series, errors='coerce')\n",
    "        \n",
    "        if numeric_vals.isna().any():\n",
    "            raise ValueError(\"Target contains non-numeric values that cannot be converted\")\n",
    "        \n",
    "        unique_vals = set(numeric_vals.dropna().unique())\n",
    "        \n",
    "        if unique_vals.issubset({0, 1}):\n",
    "            return numeric_vals.astype(int).values\n",
    "        elif unique_vals.issubset({2, 4}):\n",
    "            return (numeric_vals == 4).astype(int).values\n",
    "        elif len(unique_vals) == 2:\n",
    "            sorted_vals = sorted(unique_vals)\n",
    "            return (numeric_vals == sorted_vals[1]).astype(int).values\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot interpret target values: {unique_vals}\")\n",
    "    \n",
    "    def prepare_features(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
    "        \"\"\"Prepare feature matrix by dropping unnecessary columns.\"\"\"\n",
    "        drop_cols = [col for col in self.config.drop_columns \n",
    "                    if col in df.columns and col != target_col]\n",
    "        \n",
    "        if target_col not in df.columns:\n",
    "            raise ValueError(f\"Target column '{target_col}' not found in dataframe\")\n",
    "        \n",
    "        X_df = df.drop(columns=[target_col] + drop_cols, errors='ignore')\n",
    "        \n",
    "        for col in X_df.columns:\n",
    "            X_df[col] = pd.to_numeric(X_df[col], errors='coerce')\n",
    "        \n",
    "        X_df = X_df.dropna(axis=1, how='all')\n",
    "        \n",
    "        if X_df.shape[1] == 0:\n",
    "            raise ValueError(\"No valid features remaining after preprocessing\")\n",
    "        \n",
    "        self.feature_names = list(X_df.columns)\n",
    "        return X_df\n",
    "    \n",
    "    def handle_missing_values(self, X_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Handle missing values according to configuration.\"\"\"\n",
    "        missing_cols = X_df.columns[X_df.isna().any()].tolist()\n",
    "        \n",
    "        if not missing_cols:\n",
    "            return X_df\n",
    "        \n",
    "        self.logger.warning(f\" Missing values detected in {len(missing_cols)} columns\")\n",
    "        \n",
    "        if self.config.handle_missing == \"mean\":\n",
    "            X_df = X_df.fillna(X_df.mean())\n",
    "        elif self.config.handle_missing == \"median\":\n",
    "            X_df = X_df.fillna(X_df.median())\n",
    "        elif self.config.handle_missing == \"drop\":\n",
    "            X_df = X_df.dropna()\n",
    "        \n",
    "        return X_df\n",
    "    \n",
    "    def split_data(self, X: np.ndarray, y: np.ndarray, \n",
    "                   random_state: int) -> Tuple[np.ndarray, ...]:\n",
    "        \"\"\"Split data into train, validation, and test sets.\"\"\"\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, test_size=self.config.test_size, \n",
    "            stratify=y, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        val_size = self.config.validation_size / (1 - self.config.test_size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size,\n",
    "            stratify=y_temp, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"   Train set: {X_train.shape[0]} samples\")\n",
    "        self.logger.info(f\"   Validation set: {X_val.shape[0]} samples\")\n",
    "        self.logger.info(f\"   Test set: {X_test.shape[0]} samples\")\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \n",
    "    def scale_features(self, X_train: np.ndarray, X_val: np.ndarray, \n",
    "                      X_test: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Scale features using StandardScaler.\"\"\"\n",
    "        if not self.config.scale_features:\n",
    "            return X_train, X_val, X_test\n",
    "        \n",
    "        with timer(self.logger, \"Feature scaling\"):\n",
    "            self.scaler = StandardScaler()\n",
    "            X_train = self.scaler.fit_transform(X_train).astype(np.float32)\n",
    "            X_val = self.scaler.transform(X_val).astype(np.float32)\n",
    "            X_test = self.scaler.transform(X_test).astype(np.float32)\n",
    "        \n",
    "        return X_train, X_val, X_test\n",
    "    \n",
    "    def load_and_preprocess(self, filepath: Path, random_state: int) -> Tuple[np.ndarray, ...]:\n",
    "        \"\"\"Complete data loading and preprocessing pipeline.\"\"\"\n",
    "        df = self.load_dataset(filepath)\n",
    "        target_col = self.identify_target_column(df)\n",
    "        X_df = self.prepare_features(df, target_col)\n",
    "        X_df = self.handle_missing_values(X_df)\n",
    "        y = self.preprocess_target(df[target_col])\n",
    "        \n",
    "        if X_df.shape[0] != len(y):\n",
    "            raise ValueError(f\"Shape mismatch: X has {X_df.shape[0]} samples but y has {len(y)}\")\n",
    "        \n",
    "        benign_count = np.sum(y == 0)\n",
    "        malignant_count = np.sum(y == 1)\n",
    "        self.logger.info(f\"   Class distribution: Benign={benign_count} ({benign_count/len(y)*100:.1f}%), \"\n",
    "                        f\"Malignant={malignant_count} ({malignant_count/len(y)*100:.1f}%)\")\n",
    "        self.logger.info(f\"   Features: {len(self.feature_names)}\")\n",
    "        \n",
    "        X = X_df.values.astype(np.float32)\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = self.split_data(X, y, random_state)\n",
    "        X_train, X_val, X_test = self.scale_features(X_train, X_val, X_test)\n",
    "        \n",
    "        clear_memory()\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# =============================================================================\n",
    "# METRICS COMPUTATION \n",
    "# =============================================================================\n",
    "\n",
    "class MetricsComputer:\n",
    "    \"\"\"Computes and formats model evaluation metrics.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_all_metrics(y_true: np.ndarray, y_pred: np.ndarray, \n",
    "                           y_proba: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Compute comprehensive evaluation metrics.\"\"\"\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        auc = roc_auc_score(y_true, y_proba)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "        f1 = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0.0\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"auc\": float(auc),\n",
    "            \"sensitivity\": float(sensitivity),\n",
    "            \"specificity\": float(specificity),\n",
    "            \"precision\": float(precision),\n",
    "            \"npv\": float(npv),\n",
    "            \"f1\": float(f1),\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"roc_curve\": (fpr, tpr),\n",
    "            \"tn\": int(tn), \n",
    "            \"fp\": int(fp), \n",
    "            \"fn\": int(fn), \n",
    "            \"tp\": int(tp)\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_metrics_summary(metrics: Dict[str, Any], prefix: str = \"\") -> str:\n",
    "        \"\"\"Format metrics as a readable summary.\"\"\"\n",
    "        lines = [\n",
    "            f\"\\n {prefix}MODEL PERFORMANCE METRICS:\",\n",
    "            f\"   Accuracy:    {metrics['accuracy']:.4f}\",\n",
    "            f\"   AUC-ROC:     {metrics['auc']:.4f}\",\n",
    "            f\"   Sensitivity: {metrics['sensitivity']:.4f}\",\n",
    "            f\"   Specificity: {metrics['specificity']:.4f}\",\n",
    "            f\"   Precision:   {metrics['precision']:.4f}\",\n",
    "            f\"   NPV:         {metrics['npv']:.4f}\",\n",
    "            f\"   F1 Score:    {metrics['f1']:.4f}\",\n",
    "            \"\\nðŸ“‹ CONFUSION MATRIX:\",\n",
    "            f\"   TN: {metrics['tn']:4d}  FP: {metrics['fp']:4d}\",\n",
    "            f\"   FN: {metrics['fn']:4d}  TP: {metrics['tp']:4d}\"\n",
    "        ]\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# =============================================================================\n",
    "# MATPLOTLIB WINDOW PLOT\n",
    "# =============================================================================\n",
    "\n",
    "class WindowPlotManager:\n",
    "    \"\"\"Manages creation of matplotlib window-based plots including CV results.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TabNetConfig, logger: Logger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.figures = []\n",
    "    \n",
    "    def create_training_history_plot(self, history: Dict) -> plt.Figure:\n",
    "        \"\"\"Create training history plot in a window.\"\"\"\n",
    "        if not history:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            fig = plt.figure(figsize=(16, 10))\n",
    "            fig.canvas.manager.set_window_title('TabNet Training History')\n",
    "            gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\n",
    "            \n",
    "            epochs = list(range(1, len(history.get('loss', [])) + 1))\n",
    "            \n",
    "            # Loss plot\n",
    "            ax1 = fig.add_subplot(gs[0, 0])\n",
    "            if 'loss' in history:\n",
    "                ax1.plot(epochs, history['loss'], 'o-', label='Training Loss', linewidth=2, markersize=4)\n",
    "            if 'val_loss' in history:\n",
    "                ax1.plot(epochs, history['val_loss'], 's-', label='Validation Loss', linewidth=2, markersize=4)\n",
    "            ax1.set_xlabel('Epoch', fontsize=11)\n",
    "            ax1.set_ylabel('Loss', fontsize=11)\n",
    "            ax1.set_title('Training & Validation Loss', fontsize=13, fontweight='bold')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Accuracy plot\n",
    "            ax2 = fig.add_subplot(gs[0, 1])\n",
    "            if 'accuracy' in history:\n",
    "                ax2.plot(epochs, history['accuracy'], 'o-', label='Training Accuracy', linewidth=2, markersize=4)\n",
    "            if 'val_accuracy' in history:\n",
    "                ax2.plot(epochs, history['val_accuracy'], 's-', label='Validation Accuracy', linewidth=2, markersize=4)\n",
    "            ax2.set_xlabel('Epoch', fontsize=11)\n",
    "            ax2.set_ylabel('Accuracy', fontsize=11)\n",
    "            ax2.set_title('Training & Validation Accuracy', fontsize=13, fontweight='bold')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # AUC plot\n",
    "            ax3 = fig.add_subplot(gs[1, 0])\n",
    "            if 'auc' in history:\n",
    "                ax3.plot(epochs, history['auc'], 'o-', label='Training AUC', linewidth=2, markersize=4)\n",
    "            if 'val_auc' in history:\n",
    "                ax3.plot(epochs, history['val_auc'], 's-', label='Validation AUC', linewidth=2, markersize=4)\n",
    "            ax3.set_xlabel('Epoch', fontsize=11)\n",
    "            ax3.set_ylabel('AUC', fontsize=11)\n",
    "            ax3.set_title('Training & Validation AUC', fontsize=13, fontweight='bold')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Learning rate plot\n",
    "            ax4 = fig.add_subplot(gs[1, 1])\n",
    "            if 'lr' in history:\n",
    "                ax4.plot(epochs, history['lr'], 'o-', label='Learning Rate', linewidth=2, markersize=4, color='purple')\n",
    "            ax4.set_xlabel('Epoch', fontsize=11)\n",
    "            ax4.set_ylabel('Learning Rate', fontsize=11)\n",
    "            ax4.set_title('Learning Rate Schedule', fontsize=13, fontweight='bold')\n",
    "            ax4.legend()\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            self.figures.append(fig)\n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create training history plot: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_cv_fold_comparison_plot(self, cv_results: CVResults) -> plt.Figure:\n",
    "        \"\"\"Create comparison plot of metrics across CV folds.\"\"\"\n",
    "        if not cv_results.fold_metrics:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            metrics_to_plot = ['accuracy', 'auc', 'sensitivity', 'specificity', 'precision', 'f1']\n",
    "            n_folds = len(cv_results.fold_metrics)\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "            fig.canvas.manager.set_window_title('Cross-Validation Fold Comparison')\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            fold_numbers = list(range(1, n_folds + 1))\n",
    "            \n",
    "            for idx, metric in enumerate(metrics_to_plot):\n",
    "                ax = axes[idx]\n",
    "                \n",
    "                # Extract metric values for each fold\n",
    "                values = [fold.get(metric, 0) for fold in cv_results.fold_metrics]\n",
    "                mean_val = cv_results.mean_metrics.get(metric, 0)\n",
    "                std_val = cv_results.std_metrics.get(metric, 0)\n",
    "                \n",
    "                # Create bar plot\n",
    "                bars = ax.bar(fold_numbers, values, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "                \n",
    "                # Highlight best fold\n",
    "                if cv_results.best_fold >= 0:\n",
    "                    bars[cv_results.best_fold].set_color('gold')\n",
    "                    bars[cv_results.best_fold].set_edgecolor('darkgoldenrod')\n",
    "                    bars[cv_results.best_fold].set_linewidth(2)\n",
    "                \n",
    "                # Add mean line\n",
    "                ax.axhline(y=mean_val, color='red', linestyle='--', linewidth=2, \n",
    "                          label=f'Mean: {mean_val:.4f}')\n",
    "                \n",
    "                # Add Â±1 std band\n",
    "                ax.axhspan(mean_val - std_val, mean_val + std_val, \n",
    "                          alpha=0.2, color='red', label=f'Â±1 STD: {std_val:.4f}')\n",
    "                \n",
    "                # Formatting\n",
    "                ax.set_xlabel('Fold', fontsize=11, fontweight='bold')\n",
    "                ax.set_ylabel(metric.capitalize(), fontsize=11, fontweight='bold')\n",
    "                ax.set_title(f'{metric.capitalize()} Across Folds', fontsize=12, fontweight='bold')\n",
    "                ax.set_xticks(fold_numbers)\n",
    "                ax.legend(fontsize=9)\n",
    "                ax.grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                # Add value labels on bars\n",
    "                for i, (bar, val) in enumerate(zip(bars, values)):\n",
    "                    height = bar.get_height()\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                           f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            self.figures.append(fig)\n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create CV fold comparison plot: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_cv_boxplot(self, cv_results: CVResults) -> plt.Figure:\n",
    "        \"\"\"Create boxplot of CV metrics showing variance.\"\"\"\n",
    "        if not cv_results.fold_metrics:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            metrics_to_plot = ['accuracy', 'auc', 'sensitivity', 'specificity', 'precision', 'f1']\n",
    "            \n",
    "            # Prepare data\n",
    "            data_for_boxplot = []\n",
    "            labels = []\n",
    "            \n",
    "            for metric in metrics_to_plot:\n",
    "                values = [fold.get(metric, 0) for fold in cv_results.fold_metrics]\n",
    "                if values:\n",
    "                    data_for_boxplot.append(values)\n",
    "                    labels.append(metric.capitalize())\n",
    "            \n",
    "            if not data_for_boxplot:\n",
    "                return None\n",
    "            \n",
    "            # Create figure\n",
    "            fig, ax = plt.subplots(figsize=(14, 8))\n",
    "            fig.canvas.manager.set_window_title('Cross-Validation Metrics Distribution')\n",
    "            \n",
    "            # Create boxplot\n",
    "            bp = ax.boxplot(data_for_boxplot, labels=labels, patch_artist=True,\n",
    "                           showmeans=True, meanline=True,\n",
    "                           boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                           medianprops=dict(color='red', linewidth=2),\n",
    "                           meanprops=dict(color='green', linewidth=2, linestyle='--'),\n",
    "                           whiskerprops=dict(linewidth=1.5),\n",
    "                           capprops=dict(linewidth=1.5))\n",
    "            \n",
    "            # Customize\n",
    "            ax.set_ylabel('Score', fontsize=13, fontweight='bold')\n",
    "            ax.set_xlabel('Metric', fontsize=13, fontweight='bold')\n",
    "            ax.set_title('Distribution of Metrics Across CV Folds\\n(Red=Median, Green=Mean)', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            ax.set_ylim(0, 1.05)\n",
    "            \n",
    "            # Add legend\n",
    "            from matplotlib.lines import Line2D\n",
    "            legend_elements = [\n",
    "                Line2D([0], [0], color='red', linewidth=2, label='Median'),\n",
    "                Line2D([0], [0], color='green', linewidth=2, linestyle='--', label='Mean')\n",
    "            ]\n",
    "            ax.legend(handles=legend_elements, loc='lower right', fontsize=11)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            self.figures.append(fig)\n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create CV boxplot: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_overfitting_analysis_plot(self, cv_results: CVResults) -> plt.Figure:\n",
    "        \"\"\"Create visualization showing overfitting indicators.\"\"\"\n",
    "        if not cv_results.fold_metrics:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            analysis = cv_results.get_overfitting_analysis()\n",
    "            \n",
    "            fig = plt.figure(figsize=(16, 10))\n",
    "            fig.canvas.manager.set_window_title('Overfitting Analysis')\n",
    "            gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\n",
    "            \n",
    "            # 1. Coefficient of Variation plot\n",
    "            ax1 = fig.add_subplot(gs[0, 0])\n",
    "            if analysis['variance']:\n",
    "                metrics = list(analysis['variance'].keys())\n",
    "                cv_values = list(analysis['variance'].values())\n",
    "                \n",
    "                colors = ['red' if cv > 0.1 else 'orange' if cv > 0.05 else 'green' \n",
    "                         for cv in cv_values]\n",
    "                \n",
    "                bars = ax1.barh(metrics, cv_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "                ax1.set_xlabel('Coefficient of Variation', fontsize=11, fontweight='bold')\n",
    "                ax1.set_title('Metric Variance Across Folds\\n(Green=Low, Orange=Moderate, Red=High)', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "                ax1.axvline(x=0.05, color='orange', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "                ax1.axvline(x=0.1, color='red', linestyle='--', alpha=0.5, label='10% threshold')\n",
    "                ax1.legend(fontsize=9)\n",
    "                ax1.grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # 2. Consistency ratings\n",
    "            ax2 = fig.add_subplot(gs[0, 1])\n",
    "            if analysis['consistency']:\n",
    "                metrics = list(analysis['consistency'].keys())\n",
    "                consistency = list(analysis['consistency'].values())\n",
    "                \n",
    "                consistency_map = {'Good': 3, 'Moderate': 2, 'Poor': 1}\n",
    "                numeric_consistency = [consistency_map.get(c, 0) for c in consistency]\n",
    "                colors = ['green' if c == 'Good' else 'orange' if c == 'Moderate' else 'red' \n",
    "                         for c in consistency]\n",
    "                \n",
    "                bars = ax2.barh(metrics, numeric_consistency, color=colors, alpha=0.7, edgecolor='black')\n",
    "                ax2.set_xlabel('Consistency Level', fontsize=11, fontweight='bold')\n",
    "                ax2.set_xticks([1, 2, 3])\n",
    "                ax2.set_xticklabels(['Poor', 'Moderate', 'Good'])\n",
    "                ax2.set_title('Cross-Validation Consistency', fontsize=12, fontweight='bold')\n",
    "                ax2.grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # 3. Mean vs Std comparison\n",
    "            ax3 = fig.add_subplot(gs[1, 0])\n",
    "            metrics = ['accuracy', 'auc', 'sensitivity', 'specificity', 'precision', 'f1']\n",
    "            means = [cv_results.mean_metrics.get(m, 0) for m in metrics]\n",
    "            stds = [cv_results.std_metrics.get(m, 0) for m in metrics]\n",
    "            \n",
    "            x = np.arange(len(metrics))\n",
    "            width = 0.35\n",
    "            \n",
    "            ax3.bar(x - width/2, means, width, label='Mean', alpha=0.8, color='steelblue')\n",
    "            ax3.bar(x + width/2, stds, width, label='Std Dev', alpha=0.8, color='coral')\n",
    "            \n",
    "            ax3.set_ylabel('Value', fontsize=11, fontweight='bold')\n",
    "            ax3.set_xlabel('Metric', fontsize=11, fontweight='bold')\n",
    "            ax3.set_title('Mean Performance vs Variability', fontsize=12, fontweight='bold')\n",
    "            ax3.set_xticks(x)\n",
    "            ax3.set_xticklabels([m.capitalize() for m in metrics], rotation=45, ha='right')\n",
    "            ax3.legend(fontsize=10)\n",
    "            ax3.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # 4. Text summary of overfitting indicators\n",
    "            ax4 = fig.add_subplot(gs[1, 1])\n",
    "            ax4.axis('off')\n",
    "            \n",
    "            summary_text = \" OVERFITTING ANALYSIS SUMMARY\\n\\n\"\n",
    "            \n",
    "            if analysis['overfitting_indicators']:\n",
    "                summary_text += \" Potential Issues Detected:\\n\"\n",
    "                for indicator in analysis['overfitting_indicators'][:5]:  # Show top 5\n",
    "                    summary_text += f\"  â€¢ {indicator}\\n\"\n",
    "            else:\n",
    "                summary_text += \" No major overfitting indicators detected\\n\"\n",
    "            \n",
    "            summary_text += f\"\\n Overall Assessment:\\n\"\n",
    "            good_consistency = sum(1 for c in analysis['consistency'].values() if c == 'Good')\n",
    "            total_consistency = len(analysis['consistency'])\n",
    "            \n",
    "            if good_consistency == total_consistency:\n",
    "                summary_text += \"  â€¢ Excellent model stability\\n\"\n",
    "                summary_text += \"  â€¢ Low variance across folds\\n\"\n",
    "                summary_text += \"  â€¢ Model generalizes well\\n\"\n",
    "            elif good_consistency >= total_consistency * 0.5:\n",
    "                summary_text += \"  â€¢ Moderate model stability\\n\"\n",
    "                summary_text += \"  â€¢ Some variance present\\n\"\n",
    "                summary_text += \"  â€¢ Consider regularization\\n\"\n",
    "            else:\n",
    "                summary_text += \"  â€¢ High variance detected\\n\"\n",
    "                summary_text += \"  â€¢ Possible overfitting\\n\"\n",
    "                summary_text += \"  â€¢ Review model complexity\\n\"\n",
    "            \n",
    "            ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes,\n",
    "                    fontsize=11, verticalalignment='top', family='monospace',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            self.figures.append(fig)\n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create overfitting analysis plot: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_feature_importance_plot(self, feature_names: List[str], \n",
    "                                     importances: np.ndarray, \n",
    "                                     top_k: int = 15) -> plt.Figure:\n",
    "        \"\"\"Create feature importance plot in a window.\"\"\"\n",
    "        if len(importances) == 0:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            top_k = min(top_k, len(importances))\n",
    "            indices = np.argsort(importances)[-top_k:]\n",
    "            top_features = [feature_names[i] if i < len(feature_names) else f'Feature_{i}' \n",
    "                           for i in indices]\n",
    "            top_importances = importances[indices]\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(12, max(8, top_k * 0.4)))\n",
    "            fig.canvas.manager.set_window_title('Feature Importance')\n",
    "            \n",
    "            colors = plt.cm.viridis(np.linspace(0, 1, len(top_importances)))\n",
    "            bars = ax.barh(range(len(top_features)), top_importances, color=colors)\n",
    "            \n",
    "            ax.set_yticks(range(len(top_features)))\n",
    "            ax.set_yticklabels(top_features, fontsize=10)\n",
    "            ax.set_xlabel('Feature Importance', fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel('Features', fontsize=12, fontweight='bold')\n",
    "            ax.set_title(f'Top {top_k} Feature Importances', fontsize=14, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for i, (bar, val) in enumerate(zip(bars, top_importances)):\n",
    "                ax.text(val, i, f' {val:.4f}', va='center', fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            self.figures.append(fig)\n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create feature importance plot: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_confusion_matrix_plot(self, cm: np.ndarray) -> plt.Figure:\n",
    "        \"\"\"Create confusion matrix plot in a window.\"\"\"\n",
    "        try:\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            fig.canvas.manager.set_window_title('Confusion Matrix')\n",
    "            \n",
    "            # Normalize\n",
    "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            \n",
    "            # Create heatmap\n",
    "            im = ax.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "            ax.figure.colorbar(im, ax=ax)\n",
    "            \n",
    "            # Labels\n",
    "            classes = ['Benign', 'Malignant']\n",
    "            ax.set(xticks=np.arange(cm.shape[1]),\n",
    "                   yticks=np.arange(cm.shape[0]),\n",
    "                   xticklabels=classes, yticklabels=classes,\n",
    "                   ylabel='True Label',\n",
    "                   xlabel='Predicted Label')\n",
    "            \n",
    "            ax.set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold', pad=20)\n",
    "            \n",
    "            # Rotate the tick labels\n",
    "            plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "            \n",
    "            # Add text annotations\n",
    "            thresh = cm_normalized.max() / 2.\n",
    "            for i in range(cm.shape[0]):\n",
    "                for j in range(cm.shape[1]):\n",
    "                    ax.text(j, i, f'{cm[i, j]}\\n({cm_normalized[i, j]:.2%})',\n",
    "                           ha=\"center\", va=\"center\",\n",
    "                           color=\"white\" if cm_normalized[i, j] > thresh else \"black\",\n",
    "                           fontsize=12, fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            self.figures.append(fig)\n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create confusion matrix plot: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_roc_curve_plot(self, fpr: np.ndarray, tpr: np.ndarray, \n",
    "                            auc_score: float) -> plt.Figure:\n",
    "        \"\"\"Create ROC curve plot in a window.\"\"\"\n",
    "        try:\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            fig.canvas.manager.set_window_title('ROC Curve')\n",
    "            \n",
    "            # ROC curve\n",
    "            ax.plot(fpr, tpr, color='darkorange', lw=3, \n",
    "                   label=f'ROC curve (AUC = {auc_score:.4f})')\n",
    "            \n",
    "            # Diagonal line\n",
    "            ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "                   label='Random Classifier')\n",
    "            \n",
    "            ax.set_xlim([-0.05, 1.05])\n",
    "            ax.set_ylim([-0.05, 1.05])\n",
    "            ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "            ax.set_title('Receiver Operating Characteristic (ROC) Curve', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "            ax.legend(loc=\"lower right\", fontsize=11)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            self.figures.append(fig)\n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create ROC curve plot: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_metrics_bar_chart(self, metrics: Dict) -> plt.Figure:\n",
    "        \"\"\"Create bar chart for model metrics.\"\"\"\n",
    "        try:\n",
    "            categories = ['Accuracy', 'AUC', 'Sensitivity', 'Specificity', 'Precision', 'F1 Score']\n",
    "            values = [\n",
    "                metrics.get('accuracy', 0),\n",
    "                metrics.get('auc', 0),\n",
    "                metrics.get('sensitivity', 0),\n",
    "                metrics.get('specificity', 0),\n",
    "                metrics.get('precision', 0),\n",
    "                metrics.get('f1', 0)\n",
    "            ]\n",
    "            \n",
    "            # Create figure\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            fig.canvas.manager.set_window_title('Performance Metrics')\n",
    "            \n",
    "            # Create color map based on values\n",
    "            colors = plt.cm.RdYlGn(np.array(values))  # Red-Yellow-Green colormap\n",
    "            \n",
    "            # Create bars\n",
    "            bars = ax.bar(categories, values, color=colors, edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "            \n",
    "            # Customize plot\n",
    "            ax.set_ylabel('Score', fontsize=13, fontweight='bold')\n",
    "            ax.set_xlabel('Metric', fontsize=13, fontweight='bold')\n",
    "            ax.set_title('Model Performance Metrics', fontsize=15, fontweight='bold', pad=20)\n",
    "            ax.set_ylim(0, 1.05)\n",
    "            \n",
    "            # Add grid\n",
    "            ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "            ax.set_axisbelow(True)\n",
    "            \n",
    "            # Add value labels on top of bars\n",
    "            for bar, value in zip(bars, values):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{value:.4f}',\n",
    "                       ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "            \n",
    "            # Add horizontal reference lines\n",
    "            ax.axhline(y=0.8, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='0.8 threshold')\n",
    "            ax.axhline(y=0.9, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='0.9 threshold')\n",
    "            \n",
    "            # Rotate x-axis labels if needed\n",
    "            plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\", fontsize=11)\n",
    "            \n",
    "            # Add legend\n",
    "            ax.legend(loc='lower right', fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            self.figures.append(fig)\n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create metrics bar chart: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_and_display_plot(self, fig: plt.Figure, filepath: Path, plot_name: str) -> None:\n",
    "        \"\"\"Save plot and display in window.\"\"\"\n",
    "        if fig is None:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Save plot\n",
    "            if self.config.save_plots:\n",
    "                png_path = filepath.with_suffix('.png')\n",
    "                fig.savefig(str(png_path), dpi=self.config.plot_dpi, bbox_inches='tight')\n",
    "                self.logger.debug(f\"   Saved: {png_path.name}\")\n",
    "            \n",
    "            # Display plot - windows will show when show_all_plots() is called\n",
    "            if self.config.display_plots:\n",
    "                self.logger.debug(f\"   Prepared: {plot_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to save/display plot {filepath.name}: {e}\")\n",
    "    \n",
    "    def show_all_plots(self):\n",
    "        \"\"\"Display all created plots in separate windows.\"\"\"\n",
    "        if self.config.display_plots and self.figures:\n",
    "            self.logger.info(f\"\\nOpening {len(self.figures)} plot windows...\")\n",
    "            self.logger.info(\"   Close plot windows to continue...\")\n",
    "            plt.show(block=self.config.block_on_plot)\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINER CLASS WITH CROSS-VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "class TabNetTrainer:\n",
    "    \"\"\"Comprehensive TabNet trainer with cross-validation capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_config: TabNetConfig, data_config: DataConfig):\n",
    "        self.config = model_config\n",
    "        self.data_config = data_config\n",
    "        self.logger = Logger(\"TabNetTrainer\")\n",
    "        self.device = None\n",
    "        self.model = None\n",
    "        self.feature_names = []\n",
    "        self.results = {}\n",
    "        self.cv_results = None\n",
    "        \n",
    "        self.data_processor = DataProcessor(data_config, self.logger)\n",
    "        self.metrics_computer = MetricsComputer()\n",
    "        self.plot_manager = WindowPlotManager(model_config, self.logger)\n",
    "        \n",
    "        self._setup_environment()\n",
    "    \n",
    "    def _setup_environment(self):\n",
    "        \"\"\"Setup device and computational environment.\"\"\"\n",
    "        set_random_seeds(self.config.random_seed)\n",
    "        self.logger.info(f\" Random seeds set to {self.config.random_seed}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "            torch.backends.cudnn.deterministic = self.config.deterministic\n",
    "            torch.backends.cudnn.benchmark = not self.config.deterministic\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
    "            self.logger.info(f\" Using GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            self.logger.info(\"Using CPU\")\n",
    "            \n",
    "            self.config.batch_size = min(self.config.batch_size, 64)\n",
    "            self.config.virtual_batch_size = min(self.config.virtual_batch_size, 32)\n",
    "            self.logger.info(f\"   Adjusted batch sizes for CPU: batch={self.config.batch_size}, virtual={self.config.virtual_batch_size}\")\n",
    "    \n",
    "    def _create_model(self) -> TabNetClassifier:\n",
    "        \"\"\"Create and configure TabNet model.\"\"\"\n",
    "        if not _HAS_TABNET:\n",
    "            raise RuntimeError(\"pytorch-tabnet is not available. Install with: pip install pytorch-tabnet\")\n",
    "        \n",
    "        model = TabNetClassifier(\n",
    "            n_d=self.config.n_d,\n",
    "            n_a=self.config.n_a,\n",
    "            n_steps=self.config.n_steps,\n",
    "            gamma=self.config.gamma,\n",
    "            lambda_sparse=self.config.lambda_sparse,\n",
    "            momentum=self.config.momentum,\n",
    "            n_independent=self.config.n_independent,\n",
    "            n_shared=self.config.n_shared,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params={\"lr\": self.config.lr, \"weight_decay\": 1e-5},\n",
    "            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "            scheduler_params={\"step_size\": 10, \"gamma\": 0.9},\n",
    "            device_name=str(self.device),\n",
    "            verbose=0,  # Reduce verbosity for CV\n",
    "            seed=self.config.random_seed\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_single_fold(self, X_train: np.ndarray, X_val: np.ndarray, \n",
    "                         y_train: np.ndarray, y_val: np.ndarray,\n",
    "                         fold_idx: Optional[int] = None) -> Tuple[TabNetClassifier, Dict]:\n",
    "        \"\"\"Train model on a single fold.\"\"\"\n",
    "        model = self._create_model()\n",
    "        \n",
    "        fold_str = f\"Fold {fold_idx + 1}\" if fold_idx is not None else \"Single\"\n",
    "        \n",
    "        try:\n",
    "            with timer(self.logger, f\"{fold_str} training\"):\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    eval_name=[\"val\"],\n",
    "                    eval_metric=[\"auc\", \"accuracy\"],\n",
    "                    max_epochs=self.config.max_epochs,\n",
    "                    patience=self.config.patience,\n",
    "                    batch_size=self.config.batch_size,\n",
    "                    virtual_batch_size=self.config.virtual_batch_size,\n",
    "                    drop_last=False,\n",
    "                    num_workers=0,\n",
    "                )\n",
    "            \n",
    "            history = model.history or {}\n",
    "            self.logger.info(f\"   {fold_str} completed at epoch {model.best_epoch}\")\n",
    "            \n",
    "            return model, history\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"   {fold_str} training failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def run_cross_validation(self, X: np.ndarray, y: np.ndarray) -> CVResults:\n",
    "        \"\"\"Perform k-fold cross-validation.\"\"\"\n",
    "        with self.logger.log_section(f\"{self.config.n_folds}-FOLD CROSS-VALIDATION\"):\n",
    "            cv_results = CVResults()\n",
    "            \n",
    "            # Create stratified k-fold splitter\n",
    "            skf = StratifiedKFold(\n",
    "                n_splits=self.config.n_folds,\n",
    "                shuffle=True,\n",
    "                random_state=self.config.cv_random_state\n",
    "            )\n",
    "            \n",
    "            # Iterate through folds\n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "                self.logger.info(f\"\\n{'='*60}\")\n",
    "                self.logger.info(f\"  FOLD {fold_idx + 1}/{self.config.n_folds}\")\n",
    "                self.logger.info(f\"{'='*60}\")\n",
    "                \n",
    "                # Split data\n",
    "                X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "                y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "                \n",
    "                # Scale features\n",
    "                scaler = StandardScaler()\n",
    "                X_train_fold = scaler.fit_transform(X_train_fold).astype(np.float32)\n",
    "                X_val_fold = scaler.transform(X_val_fold).astype(np.float32)\n",
    "                \n",
    "                self.logger.info(f\"   Train: {len(y_train_fold)} samples\")\n",
    "                self.logger.info(f\"   Val:   {len(y_val_fold)} samples\")\n",
    "                \n",
    "                # Train model\n",
    "                model, history = self.train_single_fold(\n",
    "                    X_train_fold, X_val_fold, y_train_fold, y_val_fold, fold_idx\n",
    "                )\n",
    "                \n",
    "                # Evaluate on validation set\n",
    "                y_pred = model.predict(X_val_fold)\n",
    "                y_proba = model.predict_proba(X_val_fold)[:, 1]\n",
    "                \n",
    "                metrics = self.metrics_computer.compute_all_metrics(\n",
    "                    y_val_fold, y_pred, y_proba\n",
    "                )\n",
    "                \n",
    "                # Log metrics\n",
    "                self.logger.info(f\"\\n   Fold {fold_idx + 1} Results:\")\n",
    "                self.logger.info(f\"   Accuracy: {metrics['accuracy']:.4f}\")\n",
    "                self.logger.info(f\"   AUC:      {metrics['auc']:.4f}\")\n",
    "                self.logger.info(f\"   F1:       {metrics['f1']:.4f}\")\n",
    "                \n",
    "                # Store results\n",
    "                cv_results.add_fold_result(fold_idx, metrics, history)\n",
    "                \n",
    "                # Clear memory\n",
    "                del model, X_train_fold, X_val_fold\n",
    "                clear_memory()\n",
    "            \n",
    "            # Compute statistics\n",
    "            cv_results.compute_statistics()\n",
    "            \n",
    "            # Log summary\n",
    "            self.logger.info(\"\\n\" + \"=\"*70)\n",
    "            self.logger.info(\"  CROSS-VALIDATION SUMMARY\")\n",
    "            self.logger.info(\"=\"*70)\n",
    "            \n",
    "            for metric, mean_val in cv_results.mean_metrics.items():\n",
    "                std_val = cv_results.std_metrics.get(metric, 0)\n",
    "                self.logger.info(f\"   {metric.capitalize():12s}: {mean_val:.4f} Â± {std_val:.4f}\")\n",
    "            \n",
    "            self.logger.info(f\"\\n   Best Fold: {cv_results.best_fold + 1} \"\n",
    "                           f\"(AUC: {cv_results.fold_metrics[cv_results.best_fold]['auc']:.4f})\")\n",
    "            \n",
    "            # Overfitting analysis\n",
    "            analysis = cv_results.get_overfitting_analysis()\n",
    "            \n",
    "            self.logger.info(\"\\n OVERFITTING ANALYSIS:\")\n",
    "            if analysis['overfitting_indicators']:\n",
    "                self.logger.warning(\"Potential issues detected:\")\n",
    "                for indicator in analysis['overfitting_indicators']:\n",
    "                    self.logger.warning(f\"      â€¢ {indicator}\")\n",
    "            else:\n",
    "                self.logger.info(\"No major overfitting indicators detected\")\n",
    "            \n",
    "            self.logger.info(\"\\n   Consistency Ratings:\")\n",
    "            for metric, rating in analysis['consistency'].items():\n",
    "                emoji = \"\" if rating == \"Good\" else \"\" if rating == \"Moderate\" else \"\"\n",
    "                self.logger.info(f\"      {emoji} {metric.capitalize()}: {rating}\")\n",
    "            \n",
    "            return cv_results\n",
    "    \n",
    "    def train(self, X_train: np.ndarray, X_val: np.ndarray, \n",
    "             y_train: np.ndarray, y_val: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Train the TabNet model (standard or with CV).\"\"\"\n",
    "        with self.logger.log_section(\"MODEL TRAINING\"):\n",
    "            \n",
    "            if self.config.use_cross_validation:\n",
    "                # Combine train and val for CV\n",
    "                X_combined = np.vstack([X_train, X_val])\n",
    "                y_combined = np.concatenate([y_train, y_val])\n",
    "                \n",
    "                self.logger.info(\"Using Cross-Validation mode\")\n",
    "                self.cv_results = self.run_cross_validation(X_combined, y_combined)\n",
    "                \n",
    "                # Train final model on all training data\n",
    "                self.logger.info(\"\\n\" + \"=\"*70)\n",
    "                self.logger.info(\"  TRAINING FINAL MODEL ON FULL TRAINING SET\")\n",
    "                self.logger.info(\"=\"*70)\n",
    "                \n",
    "                self.model, history = self.train_single_fold(X_train, X_val, y_train, y_val)\n",
    "                \n",
    "                return history\n",
    "            else:\n",
    "                self.logger.info(\" Using standard train/val split\")\n",
    "                self.model = self._create_model()\n",
    "                \n",
    "                self.logger.info(\"Training Configuration:\")\n",
    "                self.logger.info(f\"   Max epochs: {self.config.max_epochs}\")\n",
    "                self.logger.info(f\"   Patience: {self.config.patience}\")\n",
    "                self.logger.info(f\"   Batch size: {self.config.batch_size}\")\n",
    "                self.logger.info(f\"   Virtual batch size: {self.config.virtual_batch_size}\")\n",
    "                \n",
    "                try:\n",
    "                    with timer(self.logger, \"Model training\"):\n",
    "                        self.model.fit(\n",
    "                            X_train, y_train,\n",
    "                            eval_set=[(X_val, y_val)],\n",
    "                            eval_name=[\"val\"],\n",
    "                            eval_metric=[\"auc\", \"accuracy\"],\n",
    "                            max_epochs=self.config.max_epochs,\n",
    "                            patience=self.config.patience,\n",
    "                            batch_size=self.config.batch_size,\n",
    "                            virtual_batch_size=self.config.virtual_batch_size,\n",
    "                            drop_last=False,\n",
    "                            num_workers=0,\n",
    "                        )\n",
    "                    \n",
    "                    self.logger.info(f\"Training completed at epoch {self.model.best_epoch}\")\n",
    "                    return self.model.history or {}\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Training failed: {e}\")\n",
    "                    raise\n",
    "    \n",
    "    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate the trained model.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model must be trained before evaluation\")\n",
    "        \n",
    "        with self.logger.log_section(\"MODEL EVALUATION ON TEST SET\"):\n",
    "            with timer(self.logger, \"Model evaluation\"):\n",
    "                y_pred = self.model.predict(X_test)\n",
    "                y_proba = self.model.predict_proba(X_test)[:, 1]\n",
    "                metrics = self.metrics_computer.compute_all_metrics(y_test, y_pred, y_proba)\n",
    "            \n",
    "            self.logger.info(self.metrics_computer.format_metrics_summary(metrics, \"TEST SET \"))\n",
    "            \n",
    "            self.logger.info(\"\\n CLASSIFICATION REPORT:\")\n",
    "            print(classification_report(y_test, y_pred, \n",
    "                                      target_names=['Benign', 'Malignant'], \n",
    "                                      digits=4))\n",
    "            \n",
    "            return metrics\n",
    "    \n",
    "    def create_visualizations(self, metrics: Dict, history: Dict, \n",
    "                            X_train: np.ndarray, y_train: np.ndarray, \n",
    "                            save_dir: Path) -> None:\n",
    "        \"\"\"Create all visualizations including CV plots.\"\"\"\n",
    "        with self.logger.log_section(\"GENERATING VISUALIZATIONS\"):\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            \n",
    "            # Cross-validation plots (if CV was used)\n",
    "            if self.cv_results is not None:\n",
    "                self.logger.info(\"Creating Cross-Validation plots...\")\n",
    "                \n",
    "                # Fold comparison\n",
    "                cv_comparison_fig = self.plot_manager.create_cv_fold_comparison_plot(self.cv_results)\n",
    "                if cv_comparison_fig:\n",
    "                    self.plot_manager.save_and_display_plot(\n",
    "                        cv_comparison_fig,\n",
    "                        save_dir / f\"cv_fold_comparison_{timestamp}\",\n",
    "                        \"CV Fold Comparison\"\n",
    "                    )\n",
    "                \n",
    "                # Boxplot\n",
    "                cv_boxplot_fig = self.plot_manager.create_cv_boxplot(self.cv_results)\n",
    "                if cv_boxplot_fig:\n",
    "                    self.plot_manager.save_and_display_plot(\n",
    "                        cv_boxplot_fig,\n",
    "                        save_dir / f\"cv_metrics_boxplot_{timestamp}\",\n",
    "                        \"CV Metrics Boxplot\"\n",
    "                    )\n",
    "                \n",
    "                # Overfitting analysis\n",
    "                overfitting_fig = self.plot_manager.create_overfitting_analysis_plot(self.cv_results)\n",
    "                if overfitting_fig:\n",
    "                    self.plot_manager.save_and_display_plot(\n",
    "                        overfitting_fig,\n",
    "                        save_dir / f\"overfitting_analysis_{timestamp}\",\n",
    "                        \"Overfitting Analysis\"\n",
    "                    )\n",
    "            \n",
    "            # Standard plots\n",
    "            self.logger.info(\"Creating standard training plots...\")\n",
    "            \n",
    "            # 1. Training History\n",
    "            history_fig = self.plot_manager.create_training_history_plot(history)\n",
    "            if history_fig:\n",
    "                self.plot_manager.save_and_display_plot(\n",
    "                    history_fig, \n",
    "                    save_dir / f\"training_history_{timestamp}\",\n",
    "                    \"Training History\"\n",
    "                )\n",
    "            \n",
    "            # 2. Feature Importance\n",
    "            if self.model and hasattr(self.model, 'feature_importances_'):\n",
    "                feature_fig = self.plot_manager.create_feature_importance_plot(\n",
    "                    self.feature_names, self.model.feature_importances_\n",
    "                )\n",
    "                if feature_fig:\n",
    "                    self.plot_manager.save_and_display_plot(\n",
    "                        feature_fig, \n",
    "                        save_dir / f\"feature_importance_{timestamp}\",\n",
    "                        \"Feature Importance\"\n",
    "                    )\n",
    "            \n",
    "            # 3. Confusion Matrix\n",
    "            if 'confusion_matrix' in metrics:\n",
    "                cm_fig = self.plot_manager.create_confusion_matrix_plot(metrics['confusion_matrix'])\n",
    "                if cm_fig:\n",
    "                    self.plot_manager.save_and_display_plot(\n",
    "                        cm_fig, \n",
    "                        save_dir / f\"confusion_matrix_{timestamp}\",\n",
    "                        \"Confusion Matrix\"\n",
    "                    )\n",
    "            \n",
    "            # 4. ROC Curve\n",
    "            if 'roc_curve' in metrics:\n",
    "                fpr, tpr = metrics['roc_curve']\n",
    "                roc_fig = self.plot_manager.create_roc_curve_plot(fpr, tpr, metrics['auc'])\n",
    "                if roc_fig:\n",
    "                    self.plot_manager.save_and_display_plot(\n",
    "                        roc_fig, \n",
    "                        save_dir / f\"roc_curve_{timestamp}\",\n",
    "                        \"ROC Curve\"\n",
    "                    )\n",
    "            \n",
    "            # 5. Metrics Bar Chart\n",
    "            metrics_fig = self.plot_manager.create_metrics_bar_chart(metrics)\n",
    "            if metrics_fig:\n",
    "                self.plot_manager.save_and_display_plot(\n",
    "                    metrics_fig, \n",
    "                    save_dir / f\"metrics_barchart_{timestamp}\",\n",
    "                    \"Metrics Bar Chart\"\n",
    "                )\n",
    "            \n",
    "            # Show all plots in windows\n",
    "            self.plot_manager.show_all_plots()\n",
    "            \n",
    "            self.logger.info(\"All visualizations generated and displayed\")\n",
    "    \n",
    "    def save_experiment(self, metrics: Dict, history: Dict, save_dir: Path) -> None:\n",
    "        \"\"\"Save complete experiment results including CV results.\"\"\"\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        experiment_data = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'config': {\n",
    "                'model': asdict(self.config),\n",
    "                'data': asdict(self.data_config)\n",
    "            },\n",
    "            'metrics': {k: v for k, v in metrics.items() \n",
    "                       if k not in ['confusion_matrix', 'roc_curve']},\n",
    "            'confusion_matrix': metrics.get('confusion_matrix', np.array([])).tolist(),\n",
    "            'feature_names': self.feature_names,\n",
    "            'training_info': {\n",
    "                'best_epoch': getattr(self.model, 'best_epoch', -1) if self.model else -1,\n",
    "                'total_params': sum(p.numel() for p in self.model.network.parameters()) if self.model else 0,\n",
    "                'trainable_params': sum(p.numel() for p in self.model.network.parameters() \n",
    "                                      if p.requires_grad) if self.model else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add CV results if available\n",
    "        if self.cv_results is not None:\n",
    "            experiment_data['cross_validation'] = {\n",
    "                'n_folds': self.config.n_folds,\n",
    "                'mean_metrics': self.cv_results.mean_metrics,\n",
    "                'std_metrics': self.cv_results.std_metrics,\n",
    "                'best_fold': self.cv_results.best_fold,\n",
    "                'fold_metrics': [{k: v for k, v in fold.items() \n",
    "                                 if k not in ['confusion_matrix', 'roc_curve']} \n",
    "                                for fold in self.cv_results.fold_metrics],\n",
    "                'overfitting_analysis': self.cv_results.get_overfitting_analysis()\n",
    "            }\n",
    "        \n",
    "        results_path = save_dir / f\"experiment_results_{timestamp}.json\"\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(experiment_data, f, indent=2)\n",
    "        \n",
    "        if self.model:\n",
    "            model_path = save_dir / f\"tabnet_model_{timestamp}.pth\"\n",
    "            torch.save(self.model.network.state_dict(), model_path)\n",
    "            self.logger.info(f\" Model saved: {model_path.name}\")\n",
    "        \n",
    "        self.logger.info(f\"Results saved: {results_path.name}\")\n",
    "    \n",
    "    def run_experiment(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run complete training experiment with optional CV.\"\"\"\n",
    "        with self.logger.log_section(\"TABNET TRAINING EXPERIMENT\"):\n",
    "            try:\n",
    "                # 1. Load and preprocess data\n",
    "                with self.logger.log_section(\"DATA LOADING & PREPROCESSING\"):\n",
    "                    X_train, X_val, X_test, y_train, y_val, y_test = \\\n",
    "                        self.data_processor.load_and_preprocess(\n",
    "                            Path(self.config.dataset_path),\n",
    "                            self.config.random_seed\n",
    "                        )\n",
    "                    self.feature_names = self.data_processor.feature_names\n",
    "                \n",
    "                # 2. Train model (with or without CV)\n",
    "                history = self.train(X_train, X_val, y_train, y_val)\n",
    "                \n",
    "                # 3. Evaluate model on test set\n",
    "                metrics = self.evaluate(X_test, y_test)\n",
    "                \n",
    "                # 4. Create visualizations\n",
    "                results_dir = Path(self.config.results_dir)\n",
    "                self.create_visualizations(metrics, history, X_train, y_train, results_dir)\n",
    "                \n",
    "                # 5. Save experiment\n",
    "                self.save_experiment(metrics, history, results_dir)\n",
    "                \n",
    "                self.results = {\n",
    "                    'metrics': metrics,\n",
    "                    'history': history,\n",
    "                    'feature_names': self.feature_names,\n",
    "                    'cv_results': self.cv_results\n",
    "                }\n",
    "                \n",
    "                with self.logger.log_section(\"EXPERIMENT COMPLETED SUCCESSFULLY\"):\n",
    "                    if self.cv_results:\n",
    "                        self.logger.info(\" Cross-Validation Results:\")\n",
    "                        self.logger.info(f\"   Mean AUC:      {self.cv_results.mean_metrics.get('auc', 0):.4f} \"\n",
    "                                       f\"Â± {self.cv_results.std_metrics.get('auc', 0):.4f}\")\n",
    "                        self.logger.info(f\"   Mean Accuracy: {self.cv_results.mean_metrics.get('accuracy', 0):.4f} \"\n",
    "                                       f\"Â± {self.cv_results.std_metrics.get('accuracy', 0):.4f}\")\n",
    "                    \n",
    "                    self.logger.info(f\" Final Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "                    self.logger.info(f\" Final Test AUC-ROC:  {metrics['auc']:.4f}\")\n",
    "                    self.logger.info(f\" Results saved in: {results_dir}/\")\n",
    "                \n",
    "                return self.results\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\" Experiment failed: {e}\")\n",
    "                raise\n",
    "            finally:\n",
    "                clear_memory()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def run_tabnet_training(dataset_path: str = \"C:/Users/awwal/Desktop/MLEA_experiments/data.csv\",\n",
    "                       results_dir: str = \"tabnet_results_cv\",\n",
    "                       use_cross_validation: bool = True,\n",
    "                       n_folds: int = 5,\n",
    "                       display_plots: bool = True,\n",
    "                       block_on_plot: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run TabNet training experiment with optional cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to CSV dataset\n",
    "        results_dir: Directory to save results\n",
    "        use_cross_validation: Whether to use k-fold cross-validation\n",
    "        n_folds: Number of folds for cross-validation\n",
    "        display_plots: Whether to display plots in windows\n",
    "        block_on_plot: Whether to pause execution until windows are closed\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing metrics, history, feature names, and CV results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"  TABNET TRAINING WITH CROSS-VALIDATION & OVERFITTING DETECTION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if not _HAS_TABNET:\n",
    "        print(\"\\n pytorch-tabnet is required but not installed.\")\n",
    "        print(\"   Install with: pip install pytorch-tabnet\")\n",
    "        raise ImportError(\"pytorch-tabnet not available\")\n",
    "    \n",
    "    # Configuration\n",
    "    model_config = TabNetConfig(\n",
    "        dataset_path=dataset_path,\n",
    "        results_dir=results_dir,\n",
    "        max_epochs=50,\n",
    "        patience=10,\n",
    "        n_d=16,\n",
    "        n_a=16,\n",
    "        n_steps=5,\n",
    "        gamma=1.5,\n",
    "        lambda_sparse=0.001,\n",
    "        lr=0.02,\n",
    "        use_cross_validation=use_cross_validation,\n",
    "        n_folds=n_folds,\n",
    "        display_plots=display_plots,\n",
    "        save_plots=True,\n",
    "        plot_dpi=100,\n",
    "        block_on_plot=block_on_plot\n",
    "    )\n",
    "    \n",
    "    data_config = DataConfig(\n",
    "        target_column=\"diagnosis\",\n",
    "        test_size=0.2,\n",
    "        validation_size=0.1,\n",
    "        scale_features=True,\n",
    "        handle_missing=\"mean\"\n",
    "    )\n",
    "    \n",
    "    # Create and run trainer\n",
    "    trainer = TabNetTrainer(model_config, data_config)\n",
    "    results = trainer.run_experiment()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for command-line execution.\"\"\"\n",
    "    try:\n",
    "        results = run_tabnet_training()\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Experiment failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        get_ipython()\n",
    "        print(\"\\nðŸ”¬ Running in Jupyter/IPython environment\")\n",
    "        results = run_tabnet_training(block_on_plot=False)\n",
    "        print(\"\\n Experiment completed! Results stored in 'results' variable\")\n",
    "        \n",
    "        # Display CV summary if available\n",
    "        if results.get('cv_results'):\n",
    "            print(\"\\nCross-Validation Summary:\")\n",
    "            cv = results['cv_results']\n",
    "            for metric in ['accuracy', 'auc', 'f1']:\n",
    "                if metric in cv.mean_metrics:\n",
    "                    print(f\"   {metric.capitalize()}: {cv.mean_metrics[metric]:.4f} \"\n",
    "                          f\"Â± {cv.std_metrics[metric]:.4f}\")\n",
    "    except NameError:\n",
    "        sys.exit(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
